{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "faUD996QHrfr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import wget \n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "5EDoWlebI8_0"
      },
      "outputs": [],
      "source": [
        "SEED = 1412\n",
        "para_split_val = '_01412_'\n",
        "Home = os.getcwd()\n",
        "Bayesian_file = os.path.join(Home,'final result bayesian')\n",
        "Train_file = os.path.join(Bayesian_file,'Train.csv')\n",
        "Test_file = os.path.join(Bayesian_file,'Test.csv')\n",
        "Main_file = os.path.join(Bayesian_file,'Final Result.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "temp_csv = pd.read_csv(Train_file)\n",
        "temp_cols = temp_csv.columns\n",
        "temp_cols = [i for i in temp_cols if not i.startswith('Unnamed')]\n",
        "y_train = temp_csv['Answers']\n",
        "temp_cols.remove('Answers')\n",
        "X_train = temp_csv[temp_cols]\n",
        "\n",
        "temp_csv = pd.read_csv(Test_file)\n",
        "temp_cols = temp_csv.columns\n",
        "temp_cols = [i for i in temp_cols if not i.startswith('Unnamed')]\n",
        "y_test = temp_csv['Answers']\n",
        "temp_cols.remove('Answers')\n",
        "X_test = temp_csv[temp_cols]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    os.mkdir('Energy Results')\n",
        "except:\n",
        "    pass \n",
        "Energy_result = os.path.join(os.getcwd(),'Energy Results')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "Design_Dict = {\n",
        "    'Cuckoo' : ['poulation_size', 'pa', 'n_estimator', 'criterion', 'max_feature'], \n",
        "    'Firefly' : ['poulation_size', 'alpha', 'beta0', 'gamma', 'theta', 'n_estimator', 'criterion', 'max_feature'], \n",
        "    'Bat' : ['poulation_size', 'loudness', 'pulse_rate', 'gamma', 'alpha', 'min_frequency', 'max_frequency', 'n_estimator', 'criterion', 'max_feature'], \n",
        "    'SBA' : ['poulation_size', 'loudness', 'pulse_rate', 'epsilon', 'alpha', 'min_frequency', 'max_frequency', 'n_estimator', 'criterion', 'max_feature'], \n",
        "    'PSA' : ['poulation_size', 'c1', 'c2', 'w', 'min_velocity', 'max_velocity', 'repair_name', 'n_estimator', 'criterion', 'max_feature'], \n",
        "    'Camel' : ['poulation_size', 'burden_factor', 'death_rate', 'visibility', 'supply_init', 'endurance_init', 'min_temperature', 'max_temperature', 'n_estimator', 'criterion', 'max_feature']\n",
        "     }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['Base Result', 'Cuckoo Top 10', 'FireFly Top 10', 'Bat Top 10', 'SBA Top 10', 'PSA Top 10', 'Camel Top 10'])\n"
          ]
        }
      ],
      "source": [
        "Info_Dict = dict()\n",
        "with open(Main_file,'r') as json_file:\n",
        "    Info_Dict = json.load(json_file)\n",
        "\n",
        "#Info_Dict <- list of 10 lists each list is feature , accuracy val \n",
        "print(Info_Dict.keys())\n",
        "#print(Info_Dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "pwonar4eHrft"
      },
      "outputs": [],
      "source": [
        "import niapy\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn_nature_inspired_algorithms.model_selection import NatureInspiredSearchCV\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "CUCKOO SEARCH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "from niapy.algorithms.basic import CuckooSearch\n",
        "\n",
        "Cuckoo_df = pd.DataFrame(data={'population_size':[],'Probability_Abandonment':[],'n_estimators':[],'criterion':[],'max_features':[],'Accuracy':[],'Time_Taken (s)':[],'Energy_Consumed (J)':[],'Carbon_Produced (g)':[]})\n",
        "\n",
        "\n",
        "def Cuckoo_value_update(para_str,val,time_taken,energy_used,carbon):\n",
        "    temp_para = para_str.split(para_split_val)\n",
        "    temp_para = temp_para + [val,time_taken,energy_used,carbon]\n",
        "    Cuckoo_df.loc[len(Cuckoo_df.index)] = temp_para\n",
        "\n",
        "def Save_Cuckoo_df():\n",
        "    Cuckoo_df.to_csv(os.path.join(Energy_result,'Cuckoo_Top_10.csv'))\n",
        "\n",
        "\n",
        "def mdl_Cuckoo(para_str):\n",
        "\n",
        "    temp_para = para_str.split(para_split_val)\n",
        "    #Format f'{population_size}_01412_{pa}_01412_{n_estimator}_01412_{criterion}_01412_{max_feature}'\n",
        "\n",
        "    Algo = CuckooSearch()\n",
        "    Algo.set_parameters(population_size=int(temp_para[0]),pa=float(temp_para[1]),seed=SEED)\n",
        "\n",
        "    if temp_para[4] == 'None':\n",
        "        temp_para[4] = None\n",
        "\n",
        "    nia_mdl = NatureInspiredSearchCV(\n",
        "        estimator=RandomForestClassifier(n_estimators=int(temp_para[2]),criterion=temp_para[3],max_features=temp_para[4]),\n",
        "        param_grid={},\n",
        "        algorithm=Algo,\n",
        "        runs=1\n",
        "    )\n",
        "    return nia_mdl\n",
        "\n",
        "def Score_Cuckoo(nia_mdl):\n",
        "    nia_mdl.fit(X_train,y_train)\n",
        "    val = nia_mdl.score(X_test,y_test)\n",
        "\n",
        "    return val\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "CUCKOO SEARCH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "FIRE FLY "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "from niapy.algorithms.basic import FireflyAlgorithm\n",
        "\n",
        "Firefly_df = pd.DataFrame(data={'population_size':[],'alpha':[],'beta0' : [], 'gamma' : [], 'theta' : [], 'n_estimators':[],'criterion':[],'max_features':[],'Accuracy':[],'Time_Taken (s)':[],'Energy_Consumed (J)':[],'Carbon_Produced (g)':[]})\n",
        "\n",
        "\n",
        "def Firefly_value_update(para_str,val,time_taken,energy_used,carbon):\n",
        "    temp_para = para_str.split(para_split_val)\n",
        "    temp_para = temp_para + [val,time_taken,energy_used,carbon]\n",
        "    Firefly_df.loc[len(Firefly_df.index)] = temp_para\n",
        "\n",
        "def Save_Firefly_df():\n",
        "    Firefly_df.to_csv(os.path.join(Energy_result,'Firefly_Top_10.csv'))\n",
        "\n",
        "def mdl_Firefly(para_str):\n",
        "    temp_para = para_str.split(para_split_val)\n",
        "    #Format f'{population_size}_01412_{alpha}_01412_{beta0}_01412_{gamma}_01412_{theta}_01412_{n_estimator}_01412_{criterion}_01412_{max_feature}'\n",
        "\n",
        "    Algo = FireflyAlgorithm()\n",
        "    Algo.set_parameters(population_size=int(temp_para[0]),alpha=float(temp_para[1]),beta0=float(temp_para[2]),gamma=float(temp_para[3]),theta=float(temp_para[4]),seed=SEED)\n",
        "\n",
        "    if temp_para[-1] == 'None':\n",
        "        temp_para[-1] = None\n",
        "\n",
        "    nia_mdl = NatureInspiredSearchCV(\n",
        "        estimator=RandomForestClassifier(n_estimators=int(temp_para[5]),criterion=temp_para[6],max_features=temp_para[7]),         \n",
        "        param_grid={},\n",
        "        algorithm=Algo,\n",
        "        runs=1\n",
        "    )\n",
        "    return nia_mdl\n",
        "\n",
        "def Score_Firefly(nia_mdl):\n",
        "    nia_mdl.fit(X_train,y_train)\n",
        "    val = nia_mdl.score(X_test,y_test)\n",
        "\n",
        "    return val\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "FIRE FLY "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "BAT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "from niapy.algorithms.basic import BatAlgorithm\n",
        "\n",
        "Bat_df = pd.DataFrame(data={'population_size':[],'loudness':[],'pulse_rate':[],'gamma':[],'alpha':[],'min_frequency':[],'max_frequency':[],'n_estimators':[],'criterion':[],'max_features':[],'Accuracy':[],'Time_Taken (s)':[],'Energy_Consumed (J)':[],'Carbon_Produced (g)':[]})\n",
        "\n",
        "\n",
        "def Bat_value_update(para_str,val,time_taken,energy_used,carbon):\n",
        "    temp_para = para_str.split(para_split_val)\n",
        "    temp_para = temp_para + [val,time_taken,energy_used,carbon]\n",
        "    Bat_df.loc[len(Bat_df.index)] = temp_para\n",
        "\n",
        "def Save_Bat_df():\n",
        "    Bat_df.to_csv(os.path.join(Energy_result,'Bat_Top_10.csv'))\n",
        "\n",
        "def mdl_Bat(para_str):\n",
        "    temp_para = para_str.split(para_split_val)\n",
        "    #Format f'{population_size}_01412_{loudness}_01412_{pulse_rate}_01412_{gamma}_01412_{alpha}_01412_{min_frequency}_01412_{max_frequency}_01412_{n_estimator}_01412_{criterion}_01412_{max_feature}'\n",
        "\n",
        "    Algo = BatAlgorithm()\n",
        "    Algo.set_parameters(population_size=int(temp_para[0]),loudness=float(temp_para[1]),pulse_rate=float(temp_para[2]),alpha=float(temp_para[3]),gamma=float(temp_para[4]),min_frequency=float(temp_para[5]),max_frequency=float(temp_para[6]),seed=SEED)\n",
        "    \n",
        "    if temp_para[-1] == 'None':\n",
        "        temp_para[-1] = None\n",
        "\n",
        "    nia_mdl = NatureInspiredSearchCV(\n",
        "        estimator=RandomForestClassifier(n_estimators=int(temp_para[7]),criterion=temp_para[8],max_features=temp_para[9]),         \n",
        "        param_grid={},\n",
        "        algorithm=Algo,\n",
        "        runs=1\n",
        "    )\n",
        "    return nia_mdl\n",
        "\n",
        "def Score_Bat(nia_mdl):\n",
        "    nia_mdl.fit(X_train,y_train)\n",
        "    val = nia_mdl.score(X_test,y_test)\n",
        "    return val\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "BAT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SELF ADAPTIVE BAT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "from niapy.algorithms.modified import AdaptiveBatAlgorithm\n",
        "\n",
        "SBA_df = pd.DataFrame(data={'population_size':[],'loudness':[],'pulse_rate':[],'epsilon':[],'alpha':[],'min_frequency':[],'max_frequency':[],'n_estimators':[],'criterion':[],'max_features':[],'Accuracy':[],'Time_Taken (s)':[],'Energy_Consumed (J)':[],'Carbon_Produced (g)':[]})\n",
        "\n",
        "\n",
        "def SBA_value_update(para_str,val,time_taken,energy_used,carbon):\n",
        "    temp_para = para_str.split(para_split_val)\n",
        "    temp_para = temp_para + [val,time_taken,energy_used,carbon]\n",
        "    SBA_df.loc[len(SBA_df.index)] = temp_para\n",
        "\n",
        "def Save_SBA_df():\n",
        "    SBA_df.to_csv(os.path.join(Energy_result,'SBA_Top_10.csv'))\n",
        "\n",
        "def mdl_SBA(para_str):\n",
        "    temp_para = para_str.split(para_split_val)\n",
        "    #Format f'{population_size}_01412_{loudness}_01412_{pulse_rate}_01412_{epsilon}_01412_{alpha}_01412_{min_frequency}_01412_{max_frequency}_01412_{n_estimator}_01412_{criterion}_01412_{max_feature}'\n",
        "  \n",
        "    Algo = AdaptiveBatAlgorithm()\n",
        "    Algo.set_parameters(population_size=int(temp_para[0]),loudness=float(temp_para[1]),pulse_rate=float(temp_para[2]),alpha=float(temp_para[3]),epsilon=float(temp_para[4]),min_frequency=float(temp_para[5]),max_frequency=float(temp_para[6]),seed=SEED)\n",
        "\n",
        "    if temp_para[-1] == 'None':\n",
        "        temp_para[-1] = None\n",
        "        \n",
        "    nia_mdl = NatureInspiredSearchCV(\n",
        "        estimator=RandomForestClassifier(n_estimators=int(temp_para[7]),criterion=temp_para[8],max_features=temp_para[9]),         \n",
        "        param_grid={},\n",
        "        algorithm=Algo,\n",
        "        runs=1\n",
        "    )\n",
        "    return nia_mdl\n",
        "\n",
        "def Score_SBA(nia_mdl):\n",
        "    nia_mdl.fit(X_train,y_train)\n",
        "    val = nia_mdl.score(X_test,y_test)\n",
        "\n",
        "    return val\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SELF ADAPTIVE BAT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PARTICLE SWARM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "from niapy.algorithms.basic import ParticleSwarmAlgorithm\n",
        "\n",
        "PSA_df = pd.DataFrame(data={'population_size':[],'c1':[],'c2':[],'w':[],'min_velocity':[],'max_velocity':[],'repair_name':[],'n_estimators':[],'criterion':[],'max_features':[],'Accuracy':[],'Time_Taken (s)':[],'Energy_Consumed (J)':[],'Carbon_Produced (g)':[]})\n",
        "\n",
        "\n",
        "def PSA_value_update(para_str,val,time_taken,energy_used,carbon):\n",
        "    temp_para = para_str.split(para_split_val)\n",
        "    temp_para = temp_para + [val,time_taken,energy_used,carbon]\n",
        "    PSA_df.loc[len(PSA_df.index)] = temp_para\n",
        "\n",
        "def Save_PSA_df():\n",
        "    PSA_df.to_csv(os.path.join(Energy_result,'PSA_Top_10.csv'))\n",
        "\n",
        "def mdl_PSA(para_str):\n",
        "    temp_para = para_str.split(para_split_val)\n",
        "    #Format f'{population_size}_01412_{c1}_01412_{c2}_01412_{w}_01412_{min_velocity}_01412_{max_velocity}_01412_{repair_name}_01412_{n_estimator}_01412_{criterion}_01412_{max_feature}'\n",
        "\n",
        "    Algo = ParticleSwarmAlgorithm()\n",
        "    Algo.set_parameters(population_size=int(temp_para[0]),c1=float(temp_para[1]),c2=float(temp_para[2]),w=float(temp_para[3]),min_velocity=float(temp_para[4]),max_velocity=float(temp_para[5]),repair=eval(f'niapy.util.repair.{temp_para[6]}'),seed=SEED) \n",
        "\n",
        "    if temp_para[-1] == 'None':\n",
        "        temp_para[-1] = None\n",
        "\n",
        "    nia_mdl = NatureInspiredSearchCV(\n",
        "        estimator=RandomForestClassifier(n_estimators=int(temp_para[7]),criterion=temp_para[8],max_features=temp_para[9]),         \n",
        "        param_grid={},\n",
        "        algorithm=Algo,\n",
        "        runs=1,\n",
        "    )\n",
        "    return nia_mdl\n",
        "\n",
        "def Score_PSA(nia_mdl):\n",
        "    nia_mdl.fit(X_train,y_train)\n",
        "    val = nia_mdl.score(X_test,y_test)\n",
        "    return val\n",
        "    \n",
        "Link = 'https://www.researchgate.net/profile/Mohamed_Mourad_Lafifi/post/Which-is-the-best-swarm-size-in-PSO/attachment/5b5b6f85b53d2f89289c14e1/AS%3A653084896288769%401532718981208/download/Good+Parameters+for+Particle+Swarm+Optimization.pdf'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PARTICLE SWARM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Camel Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "from niapy.algorithms.basic import CamelAlgorithm\n",
        "\n",
        "camel_df = pd.DataFrame(data={'population_size':[],'burden_factor':[],'death_rate':[],'visibility':[],'supply_init':[],'endurance_init':[],'min_temperature':[],'max_temperature':[],'n_estimators':[],'criterion':[],'max_features':[],'Accuracy':[],'Time_Taken (s)':[],'Energy_Consumed (J)':[],'Carbon_Produced (g)':[]})\n",
        "\n",
        "\n",
        "def camel_value_update(para_str,val,time_taken,energy_used,carbon):\n",
        "    temp_para = para_str.split(para_split_val)\n",
        "    temp_para = temp_para + [val,time_taken,energy_used,carbon]\n",
        "    camel_df.loc[len(camel_df.index)] = temp_para\n",
        "\n",
        "def Save_camel_df():\n",
        "    camel_df.to_csv(os.path.join(Energy_result,'camel_Top_10.csv'))\n",
        "\n",
        "def mdl_camel(para_str):\n",
        "    temp_para = para_str.split(para_split_val)\n",
        "    #Format f'{population_size}_01412_{burden_factor}_01412_{death_rate}_01412_{visibility}_01412_{supply_init}_01412_{endurance_init}_01412_{min_temperature}_01412_{max_temperature}_01412_{n_estimator}_01412_{criterion}_01412_{max_feature}'\n",
        "\n",
        "    Algo = CamelAlgorithm()\n",
        "    Algo.set_parameters(population_size=int(temp_para[0]),burden_factor=float(temp_para[1]),death_rate=float(temp_para[2]),visibility=float(temp_para[3]),supply_init=float(temp_para[4]),endurance_init=float(temp_para[5]),min_temperature=float(temp_para[6]),max_temperature=float(temp_para[7]),seed=SEED) \n",
        "\n",
        "    if temp_para[-1] == 'None':\n",
        "        temp_para[-1] = None\n",
        "\n",
        "    nia_mdl = NatureInspiredSearchCV(\n",
        "        estimator=RandomForestClassifier(n_estimators=int(temp_para[8]),criterion=temp_para[9],max_features=temp_para[10]),         \n",
        "        param_grid={},\n",
        "        algorithm=Algo,\n",
        "        runs=1,\n",
        "    )\n",
        "\n",
        "    return nia_mdl\n",
        "\n",
        "def Score_camel(nia_mdl):\n",
        "    nia_mdl.fit(X_train,y_train)\n",
        "    val = nia_mdl.score(X_test,y_test)\n",
        "    return val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Camel Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime as dt "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "def Get_hms(val):\n",
        " \n",
        "    sec = val/1000\n",
        "    mini = sec/60 \n",
        "    hr = mini/60\n",
        "    ms = sec%1000\n",
        "    hr %= 24\n",
        "    mini %= 60\n",
        "    sec %= 60 \n",
        "\n",
        "    return int(hr), int(mini), int(sec) \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "def Check_Time(df_timestamp,h,m,s):\n",
        "    df_h,df_m,df_s = Get_hms(df_timestamp)\n",
        "    #print(f'checking {df_h}:{df_m}:{df_s}')\n",
        "    if df_h == h and df_m == m and df_s == s:\n",
        "        return True\n",
        "    return False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "def Get_req_start_Idx(s_h,s_m,s_s,prev_idx,flag=True):\n",
        "    df = pd.read_csv(energy_file)\n",
        "    for i in range(prev_idx,len(df)):\n",
        "        df_timestamp = df['TimeStamp (ms)'][i]\n",
        "        if Check_Time(df_timestamp,s_h,s_m,s_s):\n",
        "            return i\n",
        "    if flag:\n",
        "        print('re try')\n",
        "        print(prev_idx)\n",
        "        print(Get_hms(df['TimeStamp (ms)'][len(df)]))\n",
        "        return Get_req_start_Idx(s_h,s_m,s_s,prev_idx=0,flag=False)\n",
        "    else:\n",
        "        return -1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "def Cal_Energy(start_idx,e_h,e_m,e_s):\n",
        "    #total_energy_lst = []\n",
        "    total_energy = 0\n",
        "    cpu_e = 0\n",
        "    monitor_e = 0\n",
        "    disk_e = 0\n",
        "    base_e = 0\n",
        "    df = pd.read_csv(energy_file)\n",
        "#['TimeStamp (ms)', ' Total Power (W)', ' CPU (W)', ' Monitor (W)',' Disk (W)', ' Base (W)', ' Application (W)']\n",
        "    for i in range(start_idx,len(df)):\n",
        "        #total_energy_lst.append(df[' Total Power (W)'][i])\n",
        "        total_energy += df[' Total Power (W)'][i]\n",
        "        cpu_e += df[' CPU (W)'][i]\n",
        "        monitor_e += df[' Monitor (W)'][i]\n",
        "        disk_e += df[' Disk (W)'][i]\n",
        "        base_e += df[' Base (W)'][i]\n",
        "        df_timestamp = df['TimeStamp (ms)'][i]\n",
        "        if Check_Time(df_timestamp,e_h,e_m,e_s):\n",
        "            return total_energy,cpu_e,monitor_e,disk_e,base_e, i\n",
        "    return total_energy,cpu_e,monitor_e,disk_e,base_e, len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "def Get_Time_Energy(start_time,end_time,prev_idx=0):\n",
        "\n",
        "    s_d = start_time.day\n",
        "    s_h = start_time.hour\n",
        "    s_m = start_time.minute\n",
        "    s_s = start_time.second\n",
        "    #print('Start',s_d,s_h,s_m,s_s)\n",
        "    e_d = end_time.day \n",
        "    e_h = end_time.hour\n",
        "    e_m = end_time.minute\n",
        "    e_s = end_time.second\n",
        "    #print('End',e_d,e_h,e_m,e_s)\n",
        "    time_taken = 0\n",
        "    time_taken = 24*(time_taken + e_d-s_d)\n",
        "    time_taken = 60*(time_taken + e_h-s_h)\n",
        "    time_taken = 60*(time_taken + e_m - s_m)\n",
        "    time_taken = (time_taken + e_s - s_s)\n",
        "\n",
        "    start_idx = Get_req_start_Idx(s_h=s_h,s_m=s_m,s_s=s_s,prev_idx=prev_idx,flag=True)\n",
        "    if start_idx == -1:\n",
        "        print(\"FAILED TO FIND START IDX\")\n",
        "        return\n",
        "    # total_energy,cpu_e,monitor_e,disk_e, i, df[' Base (W)'][0]\n",
        "    total,cpu,monitor,disk,base,end_idx = Cal_Energy(start_idx,e_h,e_m,e_s)\n",
        "\n",
        "    return total,cpu,monitor,disk,base, time_taken, end_idx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'c:\\\\Users\\\\chand\\\\Desktop\\\\temp_here\\\\Green-Computing\\\\my_tries\\\\energy_clerk.csv'"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "energy_file = [ os.path.join(os.getcwd(),x) for x in os.listdir() if x.endswith('.csv')][0]\n",
        "energy_file\n",
        "#always check for latest copy whenever you check\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Important Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['Base Result', 'Cuckoo Top 10', 'FireFly Top 10', 'Bat Top 10', 'SBA Top 10', 'PSA Top 10', 'Camel Top 10'])\n",
            "c:\\Users\\chand\\Desktop\\temp_here\\Green-Computing\\my_tries\\energy_clerk.csv\n"
          ]
        }
      ],
      "source": [
        "print(Info_Dict.keys())\n",
        "print(energy_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Important Files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "Name_lst = ['Camel', 'PSA', 'SBA', 'Bat', 'FireFly', 'Cuckoo']\n",
        "\n",
        "save_fxn = [Save_camel_df,Save_PSA_df,Save_SBA_df,Save_Bat_df,Save_Firefly_df,Save_Cuckoo_df]\n",
        "\n",
        "update_fxn = [camel_value_update,PSA_value_update,SBA_value_update,Bat_value_update,Firefly_value_update,Cuckoo_value_update]\n",
        "#camel_value_update(para_str,val,time_taken,carbon)\n",
        "mdl_fxn = [mdl_camel, mdl_PSA, mdl_SBA,mdl_Bat,mdl_Firefly,mdl_Cuckoo]\n",
        "#mdl_camel(para_str)\n",
        "score_fxn = [Score_camel, Score_PSA, Score_SBA, Score_Bat, Score_Firefly, Score_Cuckoo]\n",
        "#Score_camel(nia_mdl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "FireFly\n",
            "92_01412_0.6888614162029947_01412_52.18801682220773_01412_0.3188231352805478_01412_0.9250069727230008_01412_603_01412_gini_01412_None\n",
            "Fitting at most 1.0 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "92_01412_0.8917689509621813_01412_75.21115868102953_01412_0.4953294691693303_01412_0.8911943611104965_01412_763_01412_gini_01412_sqrt\n",
            "Fitting at most 1.0 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "96_01412_0.2516367853464829_01412_70.13790805268793_01412_0.19782976616073777_01412_0.24484640879538802_01412_784_01412_gini_01412_sqrt\n",
            "Fitting at most 1.0 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "96_01412_0.6977176058434643_01412_87.70189108515437_01412_0.6065868016880548_01412_0.8126356408929927_01412_155_01412_gini_01412_sqrt\n",
            "Fitting at most 1.0 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "97_01412_0.339917893893679_01412_26.90229719206553_01412_0.2991060412692049_01412_0.9748137598692992_01412_442_01412_gini_01412_sqrt\n",
            "Fitting at most 1.0 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "97_01412_0.6093638083744835_01412_77.14335365372531_01412_0.1566317327912343_01412_0.4866691431631054_01412_890_01412_entropy_01412_None\n",
            "Fitting at most 1.0 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "97_01412_0.6912494292777428_01412_27.437643957648575_01412_0.9079932955207126_01412_0.7518558732210319_01412_441_01412_gini_01412_sqrt\n",
            "Fitting at most 1.0 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "97_01412_0.7479764968153081_01412_88.0352266753072_01412_0.9650044847454018_01412_0.9967386416740249_01412_155_01412_entropy_01412_log2\n",
            "Fitting at most 1.0 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "98_01412_0.19329034745811474_01412_27.18373292498957_01412_0.23804526018221936_01412_0.4970729075306286_01412_442_01412_entropy_01412_log2\n",
            "Fitting at most 1.0 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "98_01412_0.31146568589073076_01412_26.255561073029718_01412_0.49968920520769866_01412_0.49391512558050055_01412_441_01412_entropy_01412_log2\n",
            "Fitting at most 1.0 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cuckoo\n",
            "92_01412_0.35780885069155777_01412_806_01412_entropy_01412_sqrt\n",
            "Fitting at most 1.0 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "93_01412_0.17642140736590828_01412_666_01412_gini_01412_None\n",
            "Fitting at most 1.0 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "93_01412_0.8972010678062589_01412_823_01412_gini_01412_None\n",
            "Fitting at most 1.0 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "94_01412_0.2642561544111848_01412_688_01412_gini_01412_None\n",
            "Fitting at most 1.0 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "94_01412_0.43348169430894856_01412_71_01412_gini_01412_sqrt\n",
            "Fitting at most 1.0 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "94_01412_0.6454002770254591_01412_72_01412_gini_01412_log2\n",
            "Fitting at most 1.0 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "98_01412_0.3908517219941403_01412_610_01412_entropy_01412_log2\n",
            "Fitting at most 1.0 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "98_01412_0.7462923947256317_01412_610_01412_gini_01412_None\n",
            "Fitting at most 1.0 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "98_01412_0.9125578852919798_01412_610_01412_entropy_01412_sqrt\n",
            "Fitting at most 1.0 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "98_01412_0.9417252253937782_01412_610_01412_entropy_01412_log2\n",
            "Fitting at most 1.0 candidates\n",
            "Optimization finished, 1 candidates were fitted\n"
          ]
        }
      ],
      "source": [
        "My_prev_idx = 0\n",
        "\n",
        "for i in range(4,len(Name_lst)):\n",
        "   \n",
        "\n",
        "    curr_algo_name = Name_lst[i]\n",
        "    curr_save_fxn = save_fxn[i]\n",
        "    curr_update_fxn = update_fxn[i]\n",
        "    curr_mdl_fxn = mdl_fxn[i]\n",
        "    curr_score_fxn = score_fxn[i]\n",
        "\n",
        "    temp_df = pd.DataFrame(data={'para_str':[],'total (J)':[],'cpu (J)':[],'monitor (J)':[],'disk (J)':[],'base (J)':[], 'prev accuracy':[],'curr accuracy':[]})\n",
        "\n",
        "    top_10 = Info_Dict[f'{curr_algo_name} Top 10']\n",
        "    print(f'\\n\\n\\n\\n\\n{curr_algo_name}')\n",
        "\n",
        "    for curr_config in top_10:\n",
        "        para_str = curr_config[0]\n",
        "        print(para_str)\n",
        "        prev_accuracy = curr_config[-1]\n",
        "\n",
        "        curr_mdl = curr_mdl_fxn(para_str)\n",
        "        start_time = dt.now()\n",
        "        val = curr_score_fxn(curr_mdl)\n",
        "        end_time = dt.now()\n",
        "\n",
        "        total,cpu,monitor,disk,base, time_taken, My_prev_idx = Get_Time_Energy(start_time,end_time,prev_idx=My_prev_idx)\n",
        "        '''emission \n",
        "        1 kW-hr = 0.85 Kg of CO2 emission \n",
        "        36 e5 - 85 e4 mg\n",
        "        360 - 85\n",
        "        72 J - 17mg CO2 \n",
        "        1 J = (17.0/72.0) mg CO2\n",
        "        ''' \n",
        "        curr_update_fxn(para_str,val,time_taken,total,total*(17.0/72.0))\n",
        "        #(para_str,val,time_taken,energy,carbon)\n",
        "\n",
        "        temp_df.loc[len(temp_df.index)] = [para_str,total,cpu,monitor,disk,base,prev_accuracy,val]\n",
        "        #Extra\n",
        "    \n",
        "    curr_save_fxn()\n",
        "    temp_df.to_csv(os.path.join(Energy_result,f'{curr_algo_name} Extra.csv'))    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "BASEMENT DO NOT GO DOWN, I DO NOT REMEMBER WHAT IS DOWN THERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MyDict = dict()\n",
        "My_prev_idx = 0\n",
        "for M_type in range(5):\n",
        "    print(M_type)\n",
        "    for Para in range(50,501,50):\n",
        "        print(Para)\n",
        "        start_time = dt.now()\n",
        "        Name, Score = Model_try(DataFile,M_type,Para)\n",
        "        end_time = dt.now()\n",
        "        #total,cpu,monitor,disk,base, time_taken, end_idx\n",
        "        total,cpu,monitor,disk,base, time_taken, My_prev_idx = Get_Time_Energy(start_time,end_time,prev_idx=My_prev_idx)\n",
        "        TempDict = {'Total Energy (J)' : total, 'Total CPU  Energy (J)' : cpu, 'Total Monitor Energy (J)' : monitor, 'Total Disk Energy (J)' : disk, 'Base Energy (J)' : base, 'Time (Sec)' : time_taken, 'Score' : Score*100, 'n_estimators' : Para}\n",
        "        if Name in MyDict:\n",
        "            MyDict[Name].append(TempDict)\n",
        "        else:\n",
        "            MyDict[Name] = [TempDict]\n",
        "        with open('Kaito Kids Dairy.json','w') as kid:\n",
        "            json.dump(MyDict,kid)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json \n",
        "import numpy as np \n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MyDict = dict()\n",
        "with open('Kaito Kids Dairy.json','r') as kid:\n",
        "    MyDict = json.load(kid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "name_lst = []\n",
        "TE_lst = []\n",
        "CPU_lst = []\n",
        "Mon_lst = []\n",
        "Disk_lst = []\n",
        "B_lst = []\n",
        "Time_lst = []\n",
        "Acc_lst = []\n",
        "n_estimator_lst = []\n",
        "for i in MyDict.keys():\n",
        "    curr_lst = MyDict[i]\n",
        "    for j in range(len(curr_lst)):\n",
        "        inner_dict = curr_lst[j]\n",
        "        \n",
        "        name_lst.append(i)\n",
        "        TE_lst.append(inner_dict['Total Energy (J)'])\n",
        "        CPU_lst.append(inner_dict['Total CPU  Energy (J)'])\n",
        "        Mon_lst.append(inner_dict['Total Monitor Energy (J)'])\n",
        "        Disk_lst.append(inner_dict['Total Disk Energy (J)'])\n",
        "        Time_lst.append(inner_dict['Time (Sec)'])\n",
        "        B_lst.append(inner_dict['Base Energy (J)'])\n",
        "        Acc_lst.append(inner_dict['Score'])\n",
        "        n_estimator_lst.append(inner_dict['n_estimators'])\n",
        "\n",
        "\n",
        "Final_dict = {'Algo Name':name_lst, 'n_estimators' : n_estimator_lst, 'Time Taken' : Time_lst, 'Accuracy' : Acc_lst,'Total Energy (J)':TE_lst,'Base Energy (J)' : B_lst,'CPU Energy (J)' : CPU_lst, 'Monitor Energy (J)' : Mon_lst, 'Disk Energy (J)' : Disk_lst }\n",
        "\n",
        "\n",
        "df = pd.DataFrame(data=Final_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    os.mkdir('Result')\n",
        "except:\n",
        "    pass\n",
        "df.to_csv('Result\\\\Full_Result.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "algo_gap = len(MyDict.keys())\n",
        "iteration_gap = len(MyDict[list(MyDict.keys())[1]])\n",
        "algo_gap,iteration_gap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    os.mkdir('Result\\\\Algo')\n",
        "except:\n",
        "    pass\n",
        "try:\n",
        "    os.mkdir('Result\\\\Estimator')\n",
        "except:\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(algo_gap):\n",
        "    Name = Final_dict['Algo Name'][i*iteration_gap]\n",
        "    TempDict = {'n_estimators' : n_estimator_lst[i*10:(i+1)*10], 'Time Taken' : Time_lst[i*10:(i+1)*10], 'Accuracy' : Acc_lst[i*10:(i+1)*10],'Total Energy (J)':TE_lst[i*10:(i+1)*10],'Base Energy (J)' : B_lst[i*10:(i+1)*10],'CPU Energy (J)' : CPU_lst[i*10:(i+1)*10], 'Monitor Energy (J)' : Mon_lst[i*10:(i+1)*10], 'Disk Energy (J)' : Disk_lst[i*10:(i+1)*10] }\n",
        "    temp_df = pd.DataFrame(data=TempDict)\n",
        "    temp_df.to_csv(f'Result\\\\Algo\\\\{Name}.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(iteration_gap):\n",
        "    estimators = Final_dict['n_estimators'][i]\n",
        "    name_lst_temp = []\n",
        "    TE_lst_temp = []\n",
        "    CPU_lst_temp = []\n",
        "    Mon_lst_temp = []\n",
        "    Disk_lst_temp = []\n",
        "    B_lst_temp = []\n",
        "    Time_lst_temp = []\n",
        "    Acc_lst_temp = []\n",
        "    for j in range(algo_gap):\n",
        "        name_lst_temp.append(name_lst[(j*iteration_gap)+i])\n",
        "        TE_lst_temp.append(TE_lst[(j*iteration_gap)+i])\n",
        "        CPU_lst_temp.append(CPU_lst[(j*iteration_gap)+i])\n",
        "        Mon_lst_temp.append(Mon_lst[(j*iteration_gap)+i])\n",
        "        Disk_lst_temp.append(Disk_lst[(j*iteration_gap)+i])\n",
        "        B_lst_temp.append(B_lst[(j*iteration_gap)+i])\n",
        "        Time_lst_temp.append(Time_lst[(j*iteration_gap)+i])\n",
        "        Acc_lst_temp.append(Acc_lst[(j*iteration_gap)+i])\n",
        "\n",
        "    TempDict = {'Algo Name' : name_lst_temp, 'Time Taken' : Time_lst_temp, 'Accuracy' : Acc_lst_temp,'Total Energy (J)':TE_lst_temp,'Base Energy (J)' : B_lst_temp,'CPU Energy (J)' : CPU_lst_temp, 'Monitor Energy (J)' : Mon_lst_temp, 'Disk Energy (J)' : Disk_lst_temp }\n",
        "    \n",
        "    temp_df = pd.DataFrame(data=TempDict)\n",
        "    temp_df.to_csv(f'Result\\\\Estimator\\\\{estimators}.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "co2 switch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "name_lst = []\n",
        "TE_lst = []\n",
        "CPU_lst = []\n",
        "Mon_lst = []\n",
        "Disk_lst = []\n",
        "B_lst = []\n",
        "Time_lst = []\n",
        "Acc_lst = []\n",
        "n_estimator_lst = []\n",
        "for i in MyDict.keys():\n",
        "    curr_lst = MyDict[i]\n",
        "    for j in range(len(curr_lst)):\n",
        "        inner_dict = curr_lst[j]\n",
        "        \n",
        "        name_lst.append(i)\n",
        "        TE_lst.append((17.0/72.0)*inner_dict['Total Energy (J)'])\n",
        "        CPU_lst.append((17.0/72.0)*inner_dict['Total CPU  Energy (J)'])\n",
        "        Mon_lst.append((17.0/72.0)*inner_dict['Total Monitor Energy (J)'])\n",
        "        Disk_lst.append((17.0/72.0)*inner_dict['Total Disk Energy (J)'])\n",
        "        Time_lst.append(inner_dict['Time (Sec)'])\n",
        "        B_lst.append((17.0/72.0)*inner_dict['Base Energy (J)'])\n",
        "        Acc_lst.append(inner_dict['Score'])\n",
        "        n_estimator_lst.append(inner_dict['n_estimators'])\n",
        "\n",
        "\n",
        "Final_dict = {'Algo Name':name_lst, 'n_estimators' : n_estimator_lst, 'Time Taken' : Time_lst, 'Accuracy' : Acc_lst,'Total CO2 Emission (mg)':TE_lst,'Base CO2 Emission (mg)' : B_lst,'CPU CO2 Emission (mg)' : CPU_lst, 'Monitor CO2 Emission (mg)' : Mon_lst, 'Disk CO2 Emission (mg)' : Disk_lst }\n",
        "\n",
        "\n",
        "df = pd.DataFrame(data=Final_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    os.mkdir('ResultCO2')\n",
        "except:\n",
        "    pass\n",
        "df.to_csv('ResultCO2\\\\Full_Result.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "algo_gap = len(MyDict.keys())\n",
        "iteration_gap = len(MyDict[list(MyDict.keys())[1]])\n",
        "algo_gap,iteration_gap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    os.mkdir('ResultCO2\\\\Algo')\n",
        "except:\n",
        "    pass\n",
        "try:\n",
        "    os.mkdir('ResultCO2\\\\Estimator')\n",
        "except:\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(algo_gap):\n",
        "    Name = Final_dict['Algo Name'][i*iteration_gap]\n",
        "    TempDict = {'n_estimators' : n_estimator_lst[i*10:(i+1)*10], 'Time Taken' : Time_lst[i*10:(i+1)*10], 'Accuracy' : Acc_lst[i*10:(i+1)*10],'Total CO2 Emission (mg)':TE_lst[i*10:(i+1)*10],'Base CO2 Emission (mg)' : B_lst[i*10:(i+1)*10],'CPU CO2 Emission (mg)' : CPU_lst[i*10:(i+1)*10], 'Monitor CO2 Emission (mg)' : Mon_lst[i*10:(i+1)*10], 'Disk CO2 Emission (mg)' : Disk_lst[i*10:(i+1)*10] }\n",
        "    temp_df = pd.DataFrame(data=TempDict)\n",
        "    temp_df.to_csv(f'ResultCO2\\\\Algo\\\\{Name}.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(iteration_gap):\n",
        "    estimators = Final_dict['n_estimators'][i]\n",
        "    name_lst_temp = []\n",
        "    TE_lst_temp = []\n",
        "    CPU_lst_temp = []\n",
        "    Mon_lst_temp = []\n",
        "    Disk_lst_temp = []\n",
        "    B_lst_temp = []\n",
        "    Time_lst_temp = []\n",
        "    Acc_lst_temp = []\n",
        "    for j in range(algo_gap):\n",
        "        name_lst_temp.append(name_lst[(j*iteration_gap)+i])\n",
        "        TE_lst_temp.append(TE_lst[(j*iteration_gap)+i])\n",
        "        CPU_lst_temp.append(CPU_lst[(j*iteration_gap)+i])\n",
        "        Mon_lst_temp.append(Mon_lst[(j*iteration_gap)+i])\n",
        "        Disk_lst_temp.append(Disk_lst[(j*iteration_gap)+i])\n",
        "        B_lst_temp.append(B_lst[(j*iteration_gap)+i])\n",
        "        Time_lst_temp.append(Time_lst[(j*iteration_gap)+i])\n",
        "        Acc_lst_temp.append(Acc_lst[(j*iteration_gap)+i])\n",
        "\n",
        "    TempDict = {'Algo Name' : name_lst_temp, 'Time Taken' : Time_lst_temp, 'Accuracy' : Acc_lst_temp,'Total CO2 Emission (mg)':TE_lst_temp,'Base CO2 Emission (mg)' : B_lst_temp,'CPU CO2 Emission (mg)' : CPU_lst_temp, 'Monitor CO2 Emission (mg)' : Mon_lst_temp, 'Disk CO2 Emission (mg)' : Disk_lst_temp }\n",
        "    \n",
        "    temp_df = pd.DataFrame(data=TempDict)\n",
        "    temp_df.to_csv(f'ResultCO2\\\\Estimator\\\\{estimators}.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Basement!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Don't go down there!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are only faulty or once used code chunks!!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "score_lst = []\n",
        "estimator_lst = []\n",
        "para_lst = []\n",
        "name_lst = []\n",
        "\n",
        "print(score_lst[-1],estimator_lst[-1],para_lst[-1],name_lst[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-d3IvUgPHrfw",
        "outputId": "99c01bdb-d461-446a-b426-182e7149f33f"
      },
      "outputs": [],
      "source": [
        "score_lst = []\n",
        "estimator_lst = []\n",
        "para_lst = []\n",
        "name_lst = []\n",
        "MyDict = dict()\n",
        "for i in range(5):\n",
        "    ADict, NIA_Name = Model_try(DataFile,i)\n",
        "    MyDict[NIA_Name] = ADict\n",
        "    para_lst.append(ADict['Best Para'])\n",
        "    score_lst.append(ADict['Best Score'])\n",
        "    estimator_lst.append(ADict['Best Estimator'])\n",
        "    name_lst.append(NIA_Name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpAlP65xMi-7"
      },
      "outputs": [],
      "source": [
        "k_lst = list(MyDict.keys())\n",
        "k2_lst = list(MyDict[k_lst[0]].keys())\n",
        "for i in k_lst:\n",
        "  MyDict[i]['Best Estimator'] = None \n",
        "\n",
        "with open(\"NIA_Dicts.json\", \"w\") as outfile:\n",
        "    json.dump(MyDict,outfile) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRdlezKR12gB"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(data= { 'NIA' : name_lst, 'Accuracy' : score_lst, 'Estimator' : estimator_lst, 'Para' : para_lst } )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfYqRPR51TN1"
      },
      "outputs": [],
      "source": [
        "df.to_csv('Final Result.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "DON'T BE UP RUNNING IT PAST 00:00!!\n",
        "YOU SHOULD BE IN BED BY THEN!!!!!!!\n",
        "\n",
        "Here is a bed time story for you \n",
        "\n",
        "\n",
        "She carefully assured her child that there wasn't a monster under the bed, turned off the light and left.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Thanks to her, I now get to enjoy the feast I'd been craving for so long.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 64-bit (microsoft store)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "bcac82fb26a6e7c950421e78519edb87e1a5e005e0aec2fc293e679965bb2493"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
