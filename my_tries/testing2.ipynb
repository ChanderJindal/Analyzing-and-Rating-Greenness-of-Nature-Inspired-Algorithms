{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split ,cross_val_score\n",
    "#import gdown\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "import numpy as np \n",
    "SEED = 1412 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "My_data = make_regression(n_samples=10000,n_features=5,noise=20,shuffle=False,random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = My_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(X),type(y))\n",
    "print(np.shape(X),np.shape(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "My_df = pd.DataFrame(X)\n",
    "My_df['Answer'] = y \n",
    "My_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = My_df.corr()\n",
    "#Plot figsize\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "#Generate Heat Map, allow annotations and place floats in map\n",
    "sns.heatmap(corr, cmap='RdBu', annot=True, fmt=\".2f\")\n",
    "#Apply xticks\n",
    "plt.xticks(range(len(corr.columns)), corr.columns)\n",
    "#Apply yticks\n",
    "plt.yticks(range(len(corr.columns)), corr.columns)\n",
    "#show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "LR = LinearRegression()\n",
    "SDGR = SGDRegressor()\n",
    "RFR = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from niapy.problems import Problem\n",
    "from niapy.task import OptimizationType, Task\n",
    "from niapy.algorithms.modified import HybridBatAlgorithm\n",
    "\n",
    "\n",
    "def get_hyperparameters(x):\n",
    "    \"\"\"\n",
    "    Function to get hyperparameters from x value.\n",
    "    base_estimator, n_estimators, learning_rate, and loss\n",
    "    \"\"\"\n",
    "    print(f'x -> \\n{x}')\n",
    "    \n",
    "    bases = (LR, SDGR, RFR)\n",
    "    base_estimator = bases[int(x[0] * (len(bases)-1))]\n",
    "    n_estimators = int(x[1] * 100 + 1)\n",
    "    learning_rate = x[2] * 3 + 0.000001\n",
    "    losses = ('linear', 'square', 'exponential')\n",
    "    loss = losses[int(x[3] * 2)]\n",
    "\n",
    "\n",
    "    params = {\n",
    "        'base_estimator': base_estimator,\n",
    "        'n_estimators': n_estimators,\n",
    "        'learning_rate' : learning_rate,\n",
    "        'loss': loss,\n",
    "    }\n",
    "    print(f'Para -> \\n{params}')\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "def get_regressor(x):\n",
    "    \"\"\"     generate regressor berdasarkan hyperparameter yang dipilih.     \"\"\"\n",
    "    params = get_hyperparameters(x)\n",
    "    return AdaBoostRegressor(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ABRHyperparameterOptimization(Problem):\n",
    "    def __init__(self, X_train, y_train):\n",
    "        super().__init__(dimension=4, lower=0, upper=1)\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "    def _evaluate(self, x):\n",
    "        model = get_regressor(x)\n",
    "        scores = cross_val_score(model, self.X_train, self.y_train, cv=5, n_jobs=-1)\n",
    "        return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = ABRHyperparameterOptimization(X_train, y_train)\n",
    "\n",
    "# akan menjalankan maksimalisasi untuk 100 iterasi pada permasalahan dengan populasi 10\n",
    "task = Task(problem, max_iters=100, optimization_type=OptimizationType.MAXIMIZATION)\n",
    "\n",
    "algorithm = HybridBatAlgorithm(population_size=10, seed=1234)\n",
    "best_params_HBA, best_accuracy = algorithm.run(task)\n",
    " \n",
    "print('Best parameters:', get_hyperparameters(best_params_HBA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_model = AdaBoostRegressor()\n",
    "best_model_HBA = get_regressor(best_params_HBA)\n",
    "\n",
    "default_model.fit(X_train, y_train)\n",
    "best_model_HBA.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_score_train = default_model.score(X_train, y_train)\n",
    "best_score_HBA_train = best_model_HBA.score(X_train, y_train)\n",
    "\n",
    "print('Default model accuracy:', default_score_train)\n",
    "print('Best model accuracy:', best_score_HBA_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_score_test = default_model.score(X_test, y_test)\n",
    "best_score_HBA_test = best_model_HBA.score(X_test, y_test)\n",
    "\n",
    "print('Default model accuracy:', default_score_test)\n",
    "print('Best model accuracy:', best_score_HBA_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bcac82fb26a6e7c950421e78519edb87e1a5e005e0aec2fc293e679965bb2493"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
