{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "8kP5DjKoFQS7"
      },
      "outputs": [],
      "source": [
        "Link = 'https://github.com/natsunoyuki/blog_posts/blob/main/data_science/Bayesian%20Optimization%20of%20Model%20Hyperparameters.ipynb'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "BASIC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "VWCENWUzFQS9"
      },
      "outputs": [],
      "source": [
        "SEED = 1412\n",
        "Test_Ratio = 0.2\n",
        "combo_str_split_val = '_01412_'\n",
        "import os \n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Already Got the Data Folder\n",
            "Already Got this Data Folder too\n",
            "High on Energy\n"
          ]
        }
      ],
      "source": [
        "Home = os.getcwd()\n",
        "DataFolder = 'Data'\n",
        "try:\n",
        "    os.mkdir(DataFolder)\n",
        "except:\n",
        "    print('Already Got the Data Folder')\n",
        "DataFolder = os.path.join(Home,DataFolder)\n",
        "\n",
        "BayesianResultFolder = 'Bayesian_Result'\n",
        "try:\n",
        "    os.mkdir(BayesianResultFolder)\n",
        "except:\n",
        "    print('Already Got this Data Folder too')\n",
        "BayesianResultFolder = os.path.join(Home,BayesianResultFolder)\n",
        "\n",
        "Energy_result = 'Energy_Result'\n",
        "try:\n",
        "    os.mkdir(Energy_result)\n",
        "except:\n",
        "    print('High on Energy')\n",
        "Energy_result = os.path.join(Home,Energy_result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "BASIC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "DB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "nkcqKuoze_xG"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def Make_db():\n",
        "  print(\"New db\")\n",
        "  X,y = make_classification(n_samples=10000,n_features=5,n_informative=2,n_classes=2,n_clusters_per_class=1,flip_y=float(f'0.{SEED}'),shuffle=False,random_state=SEED)\n",
        "  #95 -> 4.5%\n",
        "  #90 -> 14%\n",
        "  #89 -> 14.12%\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = Test_Ratio, random_state = SEED)\n",
        "\n",
        "  Train = pd.DataFrame(X_train)\n",
        "  Train['Answers'] = y_train\n",
        "  Test = pd.DataFrame(X_test)\n",
        "  Test['Answers'] = y_test\n",
        "\n",
        "  Train_Folder = os.path.join(DataFolder,'Train.csv')\n",
        "  Test_Folder = os.path.join(DataFolder,'Test.csv')\n",
        "\n",
        "  Train.to_csv(Train_Folder)\n",
        "  Test.to_csv(Test_Folder)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "U9EhD0e0j0Oh"
      },
      "outputs": [],
      "source": [
        "if 'Train.csv' not in os.listdir(DataFolder) or 'Test.csv' not in os.listdir(DataFolder): \n",
        "  Make_db()\n",
        "\n",
        "  remove_lst = [ os.path.join(BayesianResultFolder,i) for i in os.listdir(BayesianResultFolder)]\n",
        "  for i in remove_lst:\n",
        "    os.remove(i)\n",
        "\n",
        "  remove_lst = [ os.path.join(Energy_result,i) for i in os.listdir(Energy_result)]\n",
        "  for i in remove_lst:\n",
        "    os.remove(i)\n",
        "\n",
        "#New data new results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "DB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "FILES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "Train_File = os.path.join(DataFolder,'Train.csv')\n",
        "Test_File = os.path.join(DataFolder,'Test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "sEYaYCfwWncq"
      },
      "outputs": [],
      "source": [
        "Train = pd.read_csv(Train_File)\n",
        "Test = pd.read_csv(Test_File)\n",
        "col_lst = list(Test.columns)\n",
        "col_lst.remove('Answers')\n",
        "col_lst = [i for i in col_lst if not i.startswith('Unnamed')]\n",
        "X_train = Train[col_lst]\n",
        "X_test = Test[col_lst]\n",
        "y_train = Train['Answers']\n",
        "y_test = Test['Answers']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "FILES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjwzmHJ9ei3T",
        "outputId": "0cf8e82c-2687-4ca7-a31b-666c75ebb661"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2000, 8000)"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(Test),len(Train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "hQa-0Ofoe8yQ"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np \n",
        "import niapy\n",
        "from sklearn_nature_inspired_algorithms.model_selection import NatureInspiredSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbhs_5Bhe8yS"
      },
      "source": [
        "CUCKOO SEARCH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "RPrU3DTMe8yS"
      },
      "outputs": [],
      "source": [
        "from niapy.algorithms.basic import CuckooSearch\n",
        "\n",
        "CucKoo_pbounds = {'population_size':(10,100),'pa':(0.1,1.0) ,'n_estimator':(10,1000),'criterion':(0,1),'max_feature':(0,1)}\n",
        "\n",
        "Cuckoo_df = pd.DataFrame(data={'population_size':[],'Probability_Abandonment':[],'n_estimators':[],'criterion':[],'max_features':[],'Accuracy':[],'Time_Taken (s)':[],'Energy_Consumed (J)':[],'Carbon_Produced (g)':[]})\n",
        "\n",
        "\n",
        "def Cuckoo_value_update(para_str,val,time_taken,energy_used,carbon):\n",
        "    temp_para = para_str.split(para_split_val)\n",
        "    temp_para = temp_para + [val,time_taken,energy_used,carbon]\n",
        "    Cuckoo_df.loc[len(Cuckoo_df.index)] = temp_para\n",
        "\n",
        "def Save_Cuckoo_df():\n",
        "    Cuckoo_df.to_csv(os.path.join(Energy_result,'Cuckoo_Results.csv'))\n",
        "\n",
        "\n",
        "def mdl_Cuckoo(population_size,pa,n_estimator,criterion,max_feature):\n",
        "    \n",
        "    population_size = int(population_size)\n",
        "    n_estimator = int(n_estimator)\n",
        "    if criterion < 0.5:\n",
        "        criterion = 'gini'\n",
        "    else:\n",
        "        criterion = 'entropy'\n",
        "    if max_feature < 0.34:\n",
        "        max_feature = 'sqrt'\n",
        "    elif max_feature < 0.67:\n",
        "        max_feature = 'log2'\n",
        "    else:\n",
        "        max_feature = None\n",
        "\n",
        "    combo_str = f'{population_size}_01412_{pa}_01412_{n_estimator}_01412_{criterion}_01412_{max_feature}'\n",
        "\n",
        "    Algo = CuckooSearch()\n",
        "    Algo.set_parameters(population_size=population_size,pa=pa,seed=SEED)\n",
        "\n",
        "    nia_mdl = NatureInspiredSearchCV(\n",
        "        estimator=RandomForestClassifier(n_estimators=n_estimator,criterion=criterion,max_features=max_feature),\n",
        "        param_grid={},\n",
        "        algorithm=Algo,\n",
        "        runs=1\n",
        "    )\n",
        "    return nia_mdl, combo_str"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjwskMKUe8yT"
      },
      "source": [
        "CUCKOO SEARCH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKpXd9zWe8yT"
      },
      "source": [
        "FIRE FLY "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "th9hn7Yge8yU"
      },
      "outputs": [],
      "source": [
        "from niapy.algorithms.basic import FireflyAlgorithm\n",
        "\n",
        "FireFly_pbounds = {'population_size':(10,100),'alpha':(0.1,1.0),'beta0':(0.01,100),'gamma':(0.1,1.0),'theta':(0.1,1.0),'n_estimator':(10,1000),'criterion':(0,1),'max_feature':(0,1)}\n",
        "\n",
        "Firefly_df = pd.DataFrame(data={'population_size':[],'alpha':[],'beta0' : [], 'gamma' : [], 'theta' : [], 'n_estimators':[],'criterion':[],'max_features':[],'Accuracy':[],'Time_Taken (s)':[],'Energy_Consumed (J)':[],'Carbon_Produced (g)':[]})\n",
        "\n",
        "\n",
        "def Firefly_value_update(para_str,val,time_taken,energy_used,carbon):\n",
        "    temp_para = para_str.split(para_split_val)\n",
        "    temp_para = temp_para + [val,time_taken,energy_used,carbon]\n",
        "    Firefly_df.loc[len(Firefly_df.index)] = temp_para\n",
        "\n",
        "def Save_Firefly_df():\n",
        "    Firefly_df.to_csv(os.path.join(Energy_result,'Firefly_Results.csv'))\n",
        "\n",
        "\n",
        "def mdl_Firefly(population_size,alpha,beta0,gamma,theta,n_estimator,criterion,max_feature):\n",
        "    \n",
        "    population_size = int(population_size)\n",
        "    n_estimator = int(n_estimator)\n",
        "    if criterion < 0.5:\n",
        "        criterion = 'gini'\n",
        "    else:\n",
        "        criterion = 'entropy'\n",
        "    if max_feature < 0.34:\n",
        "        max_feature = 'sqrt'\n",
        "    elif max_feature < 0.67:\n",
        "        max_feature = 'log2'\n",
        "    else:\n",
        "        max_feature = None\n",
        "\n",
        "    combo_str = f'{population_size}_01412_{alpha}_01412_{beta0}_01412_{gamma}_01412_{theta}_01412_{n_estimator}_01412_{criterion}_01412_{max_feature}'\n",
        "\n",
        "    Algo = FireflyAlgorithm()\n",
        "    Algo.set_parameters(population_size=population_size,alpha=alpha,beta0=beta0,gamma=gamma,theta=theta,seed=SEED)\n",
        "\n",
        "    nia_mdl = NatureInspiredSearchCV(\n",
        "        estimator=RandomForestClassifier(n_estimators=n_estimator,criterion=criterion,max_features=max_feature),         \n",
        "        param_grid={},\n",
        "        algorithm=Algo,\n",
        "        runs=1\n",
        "    )\n",
        "    return nia_mdl, combo_str"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xr6i5YwNe8yU"
      },
      "source": [
        "FIRE FLY "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yC4cSoSpe8yU"
      },
      "source": [
        "BAT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "FHruFa4Ae8yV"
      },
      "outputs": [],
      "source": [
        "from niapy.algorithms.basic import BatAlgorithm\n",
        "\n",
        "BAT_pbounds = {'population_size':(10,100),'loudness':(0.1,1.0),'pulse_rate':(0.1,1.0),'gamma':(0.1,1.0),'alpha':(0.1,1.0),'min_frequency':(0,10),'max_frequency':(10,100),'n_estimator':(10,1000),'criterion':(0,1),'max_feature':(0,1)}\n",
        "\n",
        "Bat_df = pd.DataFrame(data={'population_size':[],'loudness':[],'pulse_rate':[],'gamma':[],'alpha':[],'min_frequency':[],'max_frequency':[],'n_estimators':[],'criterion':[],'max_features':[],'Accuracy':[],'Time_Taken (s)':[],'Energy_Consumed (J)':[],'Carbon_Produced (g)':[]})\n",
        "\n",
        "\n",
        "def Bat_value_update(para_str,val,time_taken,energy_used,carbon):\n",
        "    temp_para = para_str.split(para_split_val)\n",
        "    temp_para = temp_para + [val,time_taken,energy_used,carbon]\n",
        "    Bat_df.loc[len(Bat_df.index)] = temp_para\n",
        "\n",
        "def Save_Bat_df():\n",
        "    Bat_df.to_csv(os.path.join(Energy_result,'Bat_Results.csv'))\n",
        "\n",
        "def mdl_Bat(population_size,loudness,pulse_rate,gamma,alpha,min_frequency,max_frequency,n_estimator,criterion,max_feature):\n",
        "    \n",
        "    population_size = int(population_size)\n",
        "    n_estimator = int(n_estimator)\n",
        "    if criterion < 0.5:\n",
        "        criterion = 'gini'\n",
        "    else:\n",
        "        criterion = 'entropy'\n",
        "    if max_feature < 0.34:\n",
        "        max_feature = 'sqrt'\n",
        "    elif max_feature < 0.67:\n",
        "        max_feature = 'log2'\n",
        "    else:\n",
        "        max_feature = None\n",
        "\n",
        "    combo_str = f'{population_size}_01412_{loudness}_01412_{pulse_rate}_01412_{gamma}_01412_{alpha}_01412_{min_frequency}_01412_{max_frequency}_01412_{n_estimator}_01412_{criterion}_01412_{max_feature}'\n",
        "\n",
        "    Algo = BatAlgorithm()\n",
        "    Algo.set_parameters(population_size=population_size,loudness=loudness,pulse_rate=pulse_rate,alpha=alpha,gamma=gamma,min_frequency=min_frequency,max_frequency=max_frequency,seed=SEED)\n",
        "\n",
        "    nia_mdl = NatureInspiredSearchCV(\n",
        "        estimator=RandomForestClassifier(n_estimators=n_estimator,criterion=criterion,max_features=max_feature),         \n",
        "        param_grid={},\n",
        "        algorithm=Algo,\n",
        "        runs=1\n",
        "    )\n",
        "    return nia_mdl, combo_str"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1x7e8ioe8yW"
      },
      "source": [
        "BAT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwQ5o3AJe8yW"
      },
      "source": [
        "SELF ADAPTIVE BAT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "_NXOmcAYe8yX"
      },
      "outputs": [],
      "source": [
        "from niapy.algorithms.modified import AdaptiveBatAlgorithm\n",
        "\n",
        "SABA_pbounds = {'population_size':(10,100),'loudness':(0.1,1.0),'pulse_rate':(0.1,1.0),'epsilon':(0,0.1),'alpha':(0.1,1.0),'min_frequency':(0,10),'max_frequency':(10,100),'n_estimator':(10,1000),'criterion':(0,1),'max_feature':(0,1)}\n",
        "\n",
        "SBA_df = pd.DataFrame(data={'population_size':[],'loudness':[],'pulse_rate':[],'epsilon':[],'alpha':[],'min_frequency':[],'max_frequency':[],'n_estimators':[],'criterion':[],'max_features':[],'Accuracy':[],'Time_Taken (s)':[],'Energy_Consumed (J)':[],'Carbon_Produced (g)':[]})\n",
        "\n",
        "\n",
        "def SBA_value_update(para_str,val,time_taken,energy_used,carbon):\n",
        "    temp_para = para_str.split(para_split_val)\n",
        "    temp_para = temp_para + [val,time_taken,energy_used,carbon]\n",
        "    SBA_df.loc[len(SBA_df.index)] = temp_para\n",
        "\n",
        "def Save_SBA_df():\n",
        "    SBA_df.to_csv(os.path.join(Energy_result,'SBA_Results.csv'))\n",
        "\n",
        "def mdl_SBA(population_size,loudness,pulse_rate,epsilon,alpha,min_frequency,max_frequency,n_estimator,criterion,max_feature):\n",
        "    \n",
        "    population_size = int(population_size)\n",
        "    n_estimator = int(n_estimator)\n",
        "    if criterion < 0.5:\n",
        "        criterion = 'gini'\n",
        "    else:\n",
        "        criterion = 'entropy'\n",
        "    if max_feature < 0.34:\n",
        "        max_feature = 'sqrt'\n",
        "    elif max_feature < 0.67:\n",
        "        max_feature = 'log2'\n",
        "    else:\n",
        "        max_feature = None\n",
        "\n",
        "    combo_str = f'{population_size}_01412_{loudness}_01412_{pulse_rate}_01412_{epsilon}_01412_{alpha}_01412_{min_frequency}_01412_{max_frequency}_01412_{n_estimator}_01412_{criterion}_01412_{max_feature}'\n",
        "\n",
        "    Algo = AdaptiveBatAlgorithm()\n",
        "    Algo.set_parameters(population_size=population_size,loudness=loudness,pulse_rate=pulse_rate,alpha=alpha,epsilon=epsilon,min_frequency=min_frequency,max_frequency=max_frequency,seed=SEED)\n",
        "\n",
        "    nia_mdl = NatureInspiredSearchCV(\n",
        "        estimator=RandomForestClassifier(n_estimators=n_estimator,criterion=criterion,max_features=max_feature),         \n",
        "        param_grid={},\n",
        "        algorithm=Algo,\n",
        "        runs=1\n",
        "    )\n",
        "    return nia_mdl, combo_str"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9EvQl5Ce8yX"
      },
      "source": [
        "SELF ADAPTIVE BAT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdK4U0Z3e8yY"
      },
      "source": [
        "PARTICLE SWARM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "F2_0l7Kme8yY"
      },
      "outputs": [],
      "source": [
        "from niapy.algorithms.basic import ParticleSwarmAlgorithm\n",
        "\n",
        "PSA_pbounds = {'population_size':(10,100),'c1':(0,4),'c2':(0,4),'w':(0,1),'min_velocity':(-10,0),'max_velocity':(0,10),'repair':(0,5),'n_estimator':(10,1000),'criterion':(0,1),'max_feature':(0,1)}\n",
        "\n",
        "PSA_df = pd.DataFrame(data={'population_size':[],'c1':[],'c2':[],'w':[],'min_velocity':[],'max_velocity':[],'repair_name':[],'n_estimators':[],'criterion':[],'max_features':[],'Accuracy':[],'Time_Taken (s)':[],'Energy_Consumed (J)':[],'Carbon_Produced (g)':[]})\n",
        "\n",
        "\n",
        "def PSA_value_update(para_str,val,time_taken,energy_used,carbon):\n",
        "    temp_para = para_str.split(para_split_val)\n",
        "    temp_para = temp_para + [val,time_taken,energy_used,carbon]\n",
        "    PSA_df.loc[len(PSA_df.index)] = temp_para\n",
        "\n",
        "def Save_PSA_df():\n",
        "    PSA_df.to_csv(os.path.join(Energy_result,'PSA_Results.csv'))\n",
        "\n",
        "def mdl_PSA(population_size,c1,c2,w,min_velocity,max_velocity,repair,n_estimator,criterion,max_feature):\n",
        "\n",
        "    population_size = int(population_size)\n",
        "    n_estimator = int(n_estimator)\n",
        "    if criterion < 0.5:\n",
        "        criterion = 'gini'\n",
        "    else:\n",
        "        criterion = 'entropy'\n",
        "    if max_feature < 0.34:\n",
        "        max_feature = 'sqrt'\n",
        "    elif max_feature < 0.67:\n",
        "        max_feature = 'log2'\n",
        "    else:\n",
        "        max_feature = None\n",
        "    #limit, limit_inverse, wang, rand, reflect\n",
        "    repair_name = ''\n",
        "    if repair < 1.0:\n",
        "        repair = niapy.util.repair.limit\n",
        "        repair_name = 'limit'\n",
        "    elif repair < 2.0:\n",
        "        repair = niapy.util.repair.limit_inverse\n",
        "        repair_name = 'limit_inverse'\n",
        "    elif repair < 3.0:\n",
        "        repair = niapy.util.repair.wang\n",
        "        repair_name = 'wang'\n",
        "    elif repair < 4.0:\n",
        "        repair = niapy.util.repair.rand\n",
        "        repair_name = 'rand'\n",
        "    else:\n",
        "        repair = niapy.util.repair.reflect\n",
        "        repair_name = 'reflect'\n",
        "\n",
        "\n",
        "    combo_str = f'{population_size}_01412_{c1}_01412_{c2}_01412_{w}_01412_{min_velocity}_01412_{max_velocity}_01412_{repair_name}_01412_{n_estimator}_01412_{criterion}_01412_{max_feature}'\n",
        "\n",
        "    Algo = ParticleSwarmAlgorithm()\n",
        "    Algo.set_parameters(population_size=population_size,c1=c1,c2=c2,w=w,min_velocity=min_velocity,max_velocity=max_velocity,repair=repair,seed=SEED) \n",
        "\n",
        "    nia_mdl = NatureInspiredSearchCV(\n",
        "        estimator=RandomForestClassifier(n_estimators=n_estimator,criterion=criterion,max_features=max_feature),         \n",
        "        param_grid={},\n",
        "        algorithm=Algo,\n",
        "        runs=1,\n",
        "    )\n",
        "    return nia_mdl, combo_str\n",
        "\n",
        "Link = 'https://www.researchgate.net/profile/Mohamed_Mourad_Lafifi/post/Which-is-the-best-swarm-size-in-PSO/attachment/5b5b6f85b53d2f89289c14e1/AS%3A653084896288769%401532718981208/download/Good+Parameters+for+Particle+Swarm+Optimization.pdf'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVoHknq7e8yY"
      },
      "source": [
        "PARTICLE SWARM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8f_g9pPjBJ4"
      },
      "source": [
        "Camel Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "C0T8P9H4jBW3"
      },
      "outputs": [],
      "source": [
        "from niapy.algorithms.basic import CamelAlgorithm\n",
        "\n",
        "camel_bounds = {'population_size':(10,100), 'burden_factor':(0.1,1.0), 'death_rate':(0.1,1.0), 'visibility':(0.1,1.0),'supply_init':(1,1000),'endurance_init':(0,1000), 'min_temperature':(-100,0), 'max_temperature':(1,100),'n_estimator':(10,1000),'criterion':(0,1),'max_feature':(0,1)}\n",
        "\n",
        "camel_df = pd.DataFrame(data={'population_size':[],'burden_factor':[],'death_rate':[],'visibility':[],'supply_init':[],'endurance_init':[],'min_temperature':[],'max_temperature':[],'n_estimators':[],'criterion':[],'max_features':[],'Accuracy':[],'Time_Taken (s)':[],'Energy_Consumed (J)':[],'Carbon_Produced (g)':[]})\n",
        "\n",
        "\n",
        "def camel_value_update(para_str,val,time_taken,energy_used,carbon):\n",
        "    temp_para = para_str.split(para_split_val)\n",
        "    temp_para = temp_para + [val,time_taken,energy_used,carbon]\n",
        "    camel_df.loc[len(camel_df.index)] = temp_para\n",
        "\n",
        "def Save_camel_df():\n",
        "    camel_df.to_csv(os.path.join(Energy_result,'Camel_Result.csv'))\n",
        "\n",
        "def mdl_camel(population_size,burden_factor,death_rate,visibility,supply_init,endurance_init,min_temperature,max_temperature,n_estimator,criterion,max_feature):\n",
        "\n",
        "    population_size = int(population_size)\n",
        "    n_estimator = int(n_estimator)\n",
        "    if criterion < 0.5:\n",
        "        criterion = 'gini'\n",
        "    else:\n",
        "        criterion = 'entropy'\n",
        "    if max_feature < 0.34:\n",
        "        max_feature = 'sqrt'\n",
        "    elif max_feature < 0.67:\n",
        "        max_feature = 'log2'\n",
        "    else:\n",
        "        max_feature = None\n",
        "\n",
        "    combo_str = f'{population_size}_01412_{burden_factor}_01412_{death_rate}_01412_{visibility}_01412_{supply_init}_01412_{endurance_init}_01412_{min_temperature}_01412_{max_temperature}_01412_{n_estimator}_01412_{criterion}_01412_{max_feature}'\n",
        "\n",
        "    Algo = CamelAlgorithm()\n",
        "    Algo.set_parameters(population_size=population_size,burden_factor=burden_factor,death_rate=death_rate,visibility=visibility,supply_init=supply_init,endurance_init=endurance_init,min_temperature=min_temperature,max_temperature=max_temperature,seed=SEED) \n",
        "\n",
        "    nia_mdl = NatureInspiredSearchCV(\n",
        "        estimator=RandomForestClassifier(n_estimators=n_estimator,criterion=criterion,max_features=max_feature),         \n",
        "        param_grid={},\n",
        "        algorithm=Algo,\n",
        "        runs=1,\n",
        "    )\n",
        "    return nia_mdl, combo_str"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVnO6a5_jBkc"
      },
      "source": [
        "Camel Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYISMKude8yZ"
      },
      "source": [
        "BAYESIAN PARAMETERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "5hKJv7nizJlS"
      },
      "outputs": [],
      "source": [
        "\n",
        "NIA_Name = {0:'Cuckoo Search',1:'Fire Fly',2:'Bat',3:'Self Adaptive Bat',4:'Particle Swarm', 5:'Camel Algorithm'}\n",
        "\n",
        "NIA_pbounds_lst = [CucKoo_pbounds, FireFly_pbounds, BAT_pbounds, SABA_pbounds, PSA_pbounds, camel_bounds]\n",
        "\n",
        "save_fxn = [Save_Cuckoo_df,Save_Firefly_df,Save_Bat_df,Save_SBA_df,Save_PSA_df,Save_camel_df]\n",
        "\n",
        "update_fxn = [Cuckoo_value_update,Firefly_value_update,Bat_value_update,SBA_value_update,PSA_value_update,camel_value_update]\n",
        "#camel_value_update(para_str,val,time_taken,carbon)\n",
        "mdl_fxn = [mdl_Cuckoo,mdl_Firefly,mdl_Bat,mdl_SBA,mdl_PSA,mdl_camel]\n",
        "#mdl_camel(para_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "BAYESIAN PARAMETERS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65Z1IZsle8yZ"
      },
      "source": [
        "FUNCTION LIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "from bayes_opt import BayesianOptimization, UtilityFunction\n",
        "from datetime import datetime as dt "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ENERGY FUNCTIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "def Get_hms(val):\n",
        " \n",
        "    sec = val/1000\n",
        "    mini = sec/60 \n",
        "    hr = mini/60\n",
        "    ms = sec%1000\n",
        "    hr %= 24\n",
        "    mini %= 60\n",
        "    sec %= 60 \n",
        "\n",
        "    return int(hr), int(mini), int(sec) \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "def Check_Time(df_timestamp,h,m,s):\n",
        "    df_h,df_m,df_s = Get_hms(df_timestamp)\n",
        "    #print(f'checking {df_h}:{df_m}:{df_s}')\n",
        "    if df_h == h and df_m == m and df_s == s:\n",
        "        return True\n",
        "    return False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "def Get_req_start_Idx(s_h,s_m,s_s,prev_idx,flag=True):\n",
        "    df = pd.read_csv(energy_file)\n",
        "    for i in range(prev_idx,len(df)):\n",
        "        df_timestamp = df['TimeStamp (ms)'][i]\n",
        "        if Check_Time(df_timestamp,s_h,s_m,s_s):\n",
        "            return i\n",
        "    if flag:\n",
        "        print('re try')\n",
        "        print(prev_idx)\n",
        "        print(Get_hms(df['TimeStamp (ms)'][len(df)]))\n",
        "        return Get_req_start_Idx(s_h,s_m,s_s,prev_idx=0,flag=False)\n",
        "    else:\n",
        "        return -1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "def Cal_Energy(start_idx,e_h,e_m,e_s):\n",
        "    #total_energy_lst = []\n",
        "    total_energy = 0\n",
        "    cpu_e = 0\n",
        "    monitor_e = 0\n",
        "    disk_e = 0\n",
        "    base_e = 0\n",
        "    df = pd.read_csv(energy_file)\n",
        "#['TimeStamp (ms)', ' Total Power (W)', ' CPU (W)', ' Monitor (W)',' Disk (W)', ' Base (W)', ' Application (W)']\n",
        "    for i in range(start_idx,len(df)):\n",
        "        #total_energy_lst.append(df[' Total Power (W)'][i])\n",
        "        total_energy += df[' Total Power (W)'][i]\n",
        "        cpu_e += df[' CPU (W)'][i]\n",
        "        monitor_e += df[' Monitor (W)'][i]\n",
        "        disk_e += df[' Disk (W)'][i]\n",
        "        base_e += df[' Base (W)'][i]\n",
        "        df_timestamp = df['TimeStamp (ms)'][i]\n",
        "        if Check_Time(df_timestamp,e_h,e_m,e_s):\n",
        "            return total_energy,cpu_e,monitor_e,disk_e,base_e, i\n",
        "    return total_energy,cpu_e,monitor_e,disk_e,base_e, len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "def Get_Time_Energy(start_time,end_time,prev_idx=0):\n",
        "\n",
        "    s_d = start_time.day\n",
        "    s_h = start_time.hour\n",
        "    s_m = start_time.minute\n",
        "    s_s = start_time.second\n",
        "    #print('Start',s_d,s_h,s_m,s_s)\n",
        "    e_d = end_time.day \n",
        "    e_h = end_time.hour\n",
        "    e_m = end_time.minute\n",
        "    e_s = end_time.second\n",
        "    #print('End',e_d,e_h,e_m,e_s)\n",
        "    time_taken = 0\n",
        "    time_taken = 24*(time_taken + e_d-s_d)\n",
        "    time_taken = 60*(time_taken + e_h-s_h)\n",
        "    time_taken = 60*(time_taken + e_m - s_m)\n",
        "    time_taken = (time_taken + e_s - s_s)\n",
        "\n",
        "    start_idx = Get_req_start_Idx(s_h=s_h,s_m=s_m,s_s=s_s,prev_idx=prev_idx,flag=True)\n",
        "    if start_idx == -1:\n",
        "        print(\"FAILED TO FIND START IDX\")\n",
        "        return\n",
        "    # total_energy,cpu_e,monitor_e,disk_e, i, df[' Base (W)'][0]\n",
        "    total,cpu,monitor,disk,base,end_idx = Cal_Energy(start_idx,e_h,e_m,e_s)\n",
        "\n",
        "    return total,cpu,monitor,disk,base, time_taken, end_idx\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ENERGY FUNCTIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'c:\\\\Users\\\\chand\\\\Desktop\\\\temp_here\\\\Green-Computing\\\\my_tries\\\\full\\\\Kid_The_Energy_Keeper.csv'"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "energy_file = [ os.path.join(os.getcwd(),x) for x in os.listdir() if x.endswith('.csv')][0]\n",
        "energy_file\n",
        "#always check for latest copy whenever you check"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "BAYESIAN FUNCTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "CagnD0eWe8ya",
        "outputId": "e711c148-352c-433e-c7de-2ba712a415b2"
      },
      "outputs": [],
      "source": [
        "def Optimize_and_plot(bounds,curr_mdl_fxn,Algo_name:str,Iters:int,curr_save_fxn,curr_update_fxn,My_prev_idx=0):\n",
        "    \n",
        "    print(f'Currently at {Algo_name}')\n",
        "\n",
        "    optimizer = BayesianOptimization(f = None, pbounds = bounds, verbose = 2, random_state = SEED)\n",
        "    utility = UtilityFunction(kind = \"ucb\", kappa = 1.96, xi = 0.01)\n",
        "\n",
        "    temp_df = pd.DataFrame(data={'para_str':[],'total (J)':[],'cpu (J)':[],'monitor (J)':[],'disk (J)':[],'base (J)':[],'Accuracy':[]})\n",
        "\n",
        "    for i in range(Iters):\n",
        "        # Get optimizer to suggest a new parameter value to try.\n",
        "        next_point = optimizer.suggest(utility)  \n",
        "        # Evaluate the output of the black_box_function using the new parameter value.\n",
        "        curr_mdl, para_str = curr_mdl_fxn(**next_point)\n",
        "\n",
        "        start_time = dt.now()\n",
        "        curr_mdl.fit(X_train,y_train)\n",
        "        val = curr_mdl.score(X_test,y_test)\n",
        "        end_time = dt.now()\n",
        "\n",
        "        total,cpu,monitor,disk,base, time_taken, My_prev_idx = Get_Time_Energy(start_time,end_time,prev_idx=My_prev_idx)\n",
        "        '''emission \n",
        "        1 kW-hr = 0.85 Kg of CO2 emission \n",
        "        36 e5 - 85 e4 mg\n",
        "        360 - 85\n",
        "        72 J - 17mg CO2 \n",
        "        1 J = (17.0/72.0) mg CO2\n",
        "        ''' \n",
        "        curr_update_fxn(para_str,val,time_taken,total,total*(17.0/72.0))\n",
        "        #(para_str,val,time_taken,energy,carbon)\n",
        "\n",
        "        temp_df.loc[len(temp_df.index)] = [para_str,total,cpu,monitor,disk,base,val]\n",
        "\n",
        "        try:\n",
        "            # Update the optimizer with the evaluation results. This needs to be in try-except\n",
        "            # to prevent repeat errors from occuring.\n",
        "            optimizer.register(params = next_point, target = target)\n",
        "        except:\n",
        "            print('What was that?')\n",
        "            pass\n",
        "\n",
        "    curr_save_fxn()\n",
        "    temp_df.to_csv(os.path.join(Energy_result,f'{Algo_name} Extra.csv'))    \n",
        "\n",
        "    plt.plot(range(1, 1+len(optimizer.space.target)), optimizer.space.target, \"-o\")\n",
        "    plt.grid(True)\n",
        "    plt.xlabel(\"Iteration\")\n",
        "    plt.ylabel(\"Score\")\n",
        "    plt.show()\n",
        "    plt.savefig(os.path.join(BayesianResultFolder,f'{Algo_name}_{Iters}_Iterations.svg')  ,format='svg')\n",
        "    plt.show()\n",
        "\n",
        "    #os.remove(energy_file)\n",
        "    #To make sure the file doesn't get like 10GB\n",
        "    #Na, it removed the col heads as well\n",
        "    return My_prev_idx \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "BAYESIAN FUNCTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "9KJEEtHze8ya"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Currently at Cuckoo Search\n",
            "Fitting at most 1.0 candidates\n",
            "Optimization finished, 1 candidates were fitted\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\chand\\Desktop\\temp_here\\Green-Computing\\my_tries\\full\\full.ipynb Cell 50\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/chand/Desktop/temp_here/Green-Computing/my_tries/full/full.ipynb#Y100sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m My_prev_idx\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/chand/Desktop/temp_here/Green-Computing/my_tries/full/full.ipynb#Y100sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m,\u001b[39mlen\u001b[39m(mdl_fxn)):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/chand/Desktop/temp_here/Green-Computing/my_tries/full/full.ipynb#Y100sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     My_prev_idx\u001b[39m=\u001b[39mOptimize_and_plot(bounds\u001b[39m=\u001b[39;49mNIA_pbounds_lst[i],curr_mdl_fxn\u001b[39m=\u001b[39;49mmdl_fxn[i],Algo_name\u001b[39m=\u001b[39;49mNIA_Name[i],curr_save_fxn\u001b[39m=\u001b[39;49msave_fxn[i],Iters\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,curr_update_fxn\u001b[39m=\u001b[39;49mupdate_fxn[i],My_prev_idx\u001b[39m=\u001b[39;49mMy_prev_idx)\n",
            "\u001b[1;32mc:\\Users\\chand\\Desktop\\temp_here\\Green-Computing\\my_tries\\full\\full.ipynb Cell 50\u001b[0m in \u001b[0;36mOptimize_and_plot\u001b[1;34m(bounds, curr_mdl_fxn, Algo_name, Iters, curr_save_fxn, curr_update_fxn, My_prev_idx)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chand/Desktop/temp_here/Green-Computing/my_tries/full/full.ipynb#Y100sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m curr_mdl, para_str \u001b[39m=\u001b[39m curr_mdl_fxn(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mnext_point)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chand/Desktop/temp_here/Green-Computing/my_tries/full/full.ipynb#Y100sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m start_time \u001b[39m=\u001b[39m dt\u001b[39m.\u001b[39mnow()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/chand/Desktop/temp_here/Green-Computing/my_tries/full/full.ipynb#Y100sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m curr_mdl\u001b[39m.\u001b[39;49mfit(X_train,y_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chand/Desktop/temp_here/Green-Computing/my_tries/full/full.ipynb#Y100sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m val \u001b[39m=\u001b[39m curr_mdl\u001b[39m.\u001b[39mscore(X_test,y_test)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chand/Desktop/temp_here/Green-Computing/my_tries/full/full.ipynb#Y100sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m end_time \u001b[39m=\u001b[39m dt\u001b[39m.\u001b[39mnow()\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn_nature_inspired_algorithms\\model_selection\\nature_inspired_search_cv.py:19\u001b[0m, in \u001b[0;36mNatureInspiredSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcv_orig \u001b[39m=\u001b[39m check_cv(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcv, y, classifier\u001b[39m=\u001b[39mis_classifier(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimator))\n\u001b[0;32m     18\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_splits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcv_orig\u001b[39m.\u001b[39mget_n_splits(X, y, groups)\n\u001b[1;32m---> 19\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mfit(X, y, groups\u001b[39m=\u001b[39mgroups, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_search.py:910\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    908\u001b[0m refit_start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m    909\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 910\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_estimator_\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    911\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    912\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_estimator_\u001b[39m.\u001b[39mfit(X, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\ensemble\\_forest.py:476\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    465\u001b[0m trees \u001b[39m=\u001b[39m [\n\u001b[0;32m    466\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_estimator(append\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[0;32m    467\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    468\u001b[0m ]\n\u001b[0;32m    470\u001b[0m \u001b[39m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    471\u001b[0m \u001b[39m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    472\u001b[0m \u001b[39m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    473\u001b[0m \u001b[39m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[39m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    475\u001b[0m \u001b[39m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 476\u001b[0m trees \u001b[39m=\u001b[39m Parallel(\n\u001b[0;32m    477\u001b[0m     n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs,\n\u001b[0;32m    478\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    479\u001b[0m     prefer\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mthreads\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    480\u001b[0m )(\n\u001b[0;32m    481\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[0;32m    482\u001b[0m         t,\n\u001b[0;32m    483\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbootstrap,\n\u001b[0;32m    484\u001b[0m         X,\n\u001b[0;32m    485\u001b[0m         y,\n\u001b[0;32m    486\u001b[0m         sample_weight,\n\u001b[0;32m    487\u001b[0m         i,\n\u001b[0;32m    488\u001b[0m         \u001b[39mlen\u001b[39;49m(trees),\n\u001b[0;32m    489\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    490\u001b[0m         class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[0;32m    491\u001b[0m         n_samples_bootstrap\u001b[39m=\u001b[39;49mn_samples_bootstrap,\n\u001b[0;32m    492\u001b[0m     )\n\u001b[0;32m    493\u001b[0m     \u001b[39mfor\u001b[39;49;00m i, t \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(trees)\n\u001b[0;32m    494\u001b[0m )\n\u001b[0;32m    496\u001b[0m \u001b[39m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    497\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_\u001b[39m.\u001b[39mextend(trees)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\joblib\\parallel.py:1046\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1044\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1046\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1047\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1050\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1051\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1052\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\joblib\\parallel.py:861\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    859\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    860\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 861\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    862\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\joblib\\parallel.py:779\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    778\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 779\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    780\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    781\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    782\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    783\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\joblib\\_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    570\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 572\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    259\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    259\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\utils\\fixes.py:117\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    116\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig):\n\u001b[1;32m--> 117\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\ensemble\\_forest.py:189\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[39melif\u001b[39;00m class_weight \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbalanced_subsample\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    187\u001b[0m         curr_sample_weight \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m compute_sample_weight(\u001b[39m\"\u001b[39m\u001b[39mbalanced\u001b[39m\u001b[39m\"\u001b[39m, y, indices\u001b[39m=\u001b[39mindices)\n\u001b[1;32m--> 189\u001b[0m     tree\u001b[39m.\u001b[39;49mfit(X, y, sample_weight\u001b[39m=\u001b[39;49mcurr_sample_weight, check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    190\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    191\u001b[0m     tree\u001b[39m.\u001b[39mfit(X, y, sample_weight\u001b[39m=\u001b[39msample_weight, check_input\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\tree\\_classes.py:969\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    939\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    940\u001b[0m     \u001b[39m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    941\u001b[0m \n\u001b[0;32m    942\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    966\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    967\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 969\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m    970\u001b[0m         X,\n\u001b[0;32m    971\u001b[0m         y,\n\u001b[0;32m    972\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    973\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[0;32m    974\u001b[0m     )\n\u001b[0;32m    975\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\tree\\_classes.py:458\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    448\u001b[0m     builder \u001b[39m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    449\u001b[0m         splitter,\n\u001b[0;32m    450\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    455\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    456\u001b[0m     )\n\u001b[1;32m--> 458\u001b[0m builder\u001b[39m.\u001b[39;49mbuild(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree_, X, y, sample_weight)\n\u001b[0;32m    460\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m is_classifier(\u001b[39mself\u001b[39m):\n\u001b[0;32m    461\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_[\u001b[39m0\u001b[39m]\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#(bounds,curr_mdl_fxn,Algo_name:str,Iters:int,curr_save_fxn,curr_update_fxn):\n",
        "My_prev_idx=0\n",
        "for i in range(0,len(mdl_fxn)):\n",
        "    My_prev_idx=Optimize_and_plot(bounds=NIA_pbounds_lst[i],curr_mdl_fxn=mdl_fxn[i],Algo_name=NIA_Name[i],curr_save_fxn=save_fxn[i],Iters=100,curr_update_fxn=update_fxn[i],My_prev_idx=My_prev_idx)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "BASE CASE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "curr_mdl = RandomForestClassifier()\n",
        "start_time = dt.now()\n",
        "curr_mdl.fit(X_train,y_train)\n",
        "val = curr_mdl.score(X_test,y_test)\n",
        "end_time = dt.now()\n",
        "\n",
        "total,cpu,monitor,disk,base, time_taken, My_prev_idx = Get_Time_Energy(start_time,end_time,prev_idx=My_prev_idx)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_dict = {'Para' : ['default'], 'Accuracy' : [val],'Time Taken (s)' :[time_taken], 'Energy Consumes (J)':[total], 'CO2 Emission (mg)' :total*(17.0/72.0)}\n",
        "df = pd.DataFrame(data=data_dict)\n",
        "df.to_csv(os.path.join(Energy_result,'Base_Result.csv'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "BASE CASE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Br_NO9vMGbt9"
      },
      "source": [
        "function ClickConnect(){\n",
        "               \n",
        "console.log(\"Working\"); \n",
        "document.querySelector(\"#comments > span\").click() \n",
        "}\n",
        "setInterval(ClickConnect,5000)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.13 64-bit (microsoft store)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "bcac82fb26a6e7c950421e78519edb87e1a5e005e0aec2fc293e679965bb2493"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
