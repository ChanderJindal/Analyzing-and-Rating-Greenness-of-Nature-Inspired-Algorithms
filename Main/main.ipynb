{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "faUD996QHrfr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import wget \n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5EDoWlebI8_0"
      },
      "outputs": [],
      "source": [
        "Home = os.getcwd()\n",
        "DataFolder = os.path.join(Home,'Data')\n",
        "df = pd.read_csv([os.path.join(DataFolder,i) for i in os.listdir(DataFolder) if i.endswith('.csv')][0])\n",
        "df_cols = list(df.columns)\n",
        "df = df.drop(columns=[i for i in df_cols if i.startswith('Unnamed:')])\n",
        "#df.to_csv(os.path.join(DataFolder,'MyDataSet.csv'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pwonar4eHrft"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jkx0Zr4GHrfv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.909144305675776\n"
          ]
        }
      ],
      "source": [
        "#df = pd.read_csv(DataFileName)\n",
        "df_cols = list(df.columns)\n",
        "y = df[df_cols[-1]]\n",
        "X = df[df_cols[:-1]]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "model = RandomForestClassifier()\n",
        "model.fit(X_train,y_train)\n",
        "val = model.score(X_test,y_test)\n",
        "print(val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime as dt "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def Get_hms(val):\n",
        " \n",
        "    sec = val/1000\n",
        "    mini = sec/60 \n",
        "    hr = mini/60\n",
        "    ms = sec%1000\n",
        "    hr %= 24\n",
        "    mini %= 60\n",
        "    sec %= 60 \n",
        "\n",
        "    return int(hr), int(mini), int(sec) \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def Check_Time(df_timestamp,h,m,s):\n",
        "    df_h,df_m,df_s = Get_hms(df_timestamp)\n",
        "    #print(f'checking {df_h}:{df_m}:{df_s}')\n",
        "    if df_h == h and df_m == m and df_s == s:\n",
        "        return True\n",
        "    return False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "def Get_req_start_Idx(s_h,s_m,s_s,prev_idx,flag=True):\n",
        "    df = pd.read_csv(energy_file)\n",
        "    for i in range(prev_idx,len(df)):\n",
        "        df_timestamp = df['TimeStamp (ms)'][i]\n",
        "        if Check_Time(df_timestamp,s_h,s_m,s_s):\n",
        "            return i\n",
        "    if flag:\n",
        "        print('re try')\n",
        "        print(prev_idx)\n",
        "        print(Get_hms(df['TimeStamp (ms)'][len(df)]))\n",
        "        return Get_req_start_Idx(s_h,s_m,s_s,prev_idx=0,flag=False)\n",
        "    else:\n",
        "        return -1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "def Cal_Energy(start_idx,e_h,e_m,e_s):\n",
        "    #total_energy_lst = []\n",
        "    total_energy = 0\n",
        "    cpu_e = 0\n",
        "    monitor_e = 0\n",
        "    disk_e = 0\n",
        "    base_e = 0\n",
        "    df = pd.read_csv(energy_file)\n",
        "#['TimeStamp (ms)', ' Total Power (W)', ' CPU (W)', ' Monitor (W)',' Disk (W)', ' Base (W)', ' Application (W)']\n",
        "    for i in range(start_idx,len(df)):\n",
        "        #total_energy_lst.append(df[' Total Power (W)'][i])\n",
        "        total_energy += df[' Total Power (W)'][i]\n",
        "        cpu_e += df[' CPU (W)'][i]\n",
        "        monitor_e += df[' Monitor (W)'][i]\n",
        "        disk_e += df[' Disk (W)'][i]\n",
        "        base_e += df[' Base (W)'][i]\n",
        "        df_timestamp = df['TimeStamp (ms)'][i]\n",
        "        if Check_Time(df_timestamp,e_h,e_m,e_s):\n",
        "            return total_energy,cpu_e,monitor_e,disk_e,base_e, i\n",
        "    return total_energy,cpu_e,monitor_e,disk_e,base_e, len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "def Get_Time_Energy(start_time,end_time,prev_idx=0):\n",
        "\n",
        "    s_d = start_time.day\n",
        "    s_h = start_time.hour\n",
        "    s_m = start_time.minute\n",
        "    s_s = start_time.second\n",
        "    #print('Start',s_d,s_h,s_m,s_s)\n",
        "    e_d = end_time.day \n",
        "    e_h = end_time.hour\n",
        "    e_m = end_time.minute\n",
        "    e_s = end_time.second\n",
        "    #print('End',e_d,e_h,e_m,e_s)\n",
        "    time_taken = 0\n",
        "    time_taken = 24*(time_taken + e_d-s_d)\n",
        "    time_taken = 60*(time_taken + e_h-s_h)\n",
        "    time_taken = 60*(time_taken + e_m - s_m)\n",
        "    time_taken = (time_taken + e_s - s_s)\n",
        "\n",
        "    start_idx = Get_req_start_Idx(s_h=s_h,s_m=s_m,s_s=s_s,prev_idx=prev_idx,flag=True)\n",
        "    if start_idx == -1:\n",
        "        print(\"FAILED TO FIND START IDX\")\n",
        "        return\n",
        "    # total_energy,cpu_e,monitor_e,disk_e, i, df[' Base (W)'][0]\n",
        "    total,cpu,monitor,disk,base,end_idx = Cal_Energy(start_idx,e_h,e_m,e_s)\n",
        "\n",
        "    return total,cpu,monitor,disk,base, time_taken, end_idx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'c:\\\\Users\\\\chand\\\\Desktop\\\\work_simple\\\\Green-Computing\\\\temp\\\\energy_clerk.csv'"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "energy_file = [ os.path.join(os.getcwd(),x) for x in os.listdir() if x.endswith('.csv')][0]\n",
        "energy_file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "50\n",
            "Fitting at most 1 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "For Grey Wolf Optimizer we get a score of 0.702229480388142.\n",
            "100\n",
            "Fitting at most 1 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "For Grey Wolf Optimizer we get a score of 0.7206528776141269.\n",
            "150\n",
            "Fitting at most 1 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "For Grey Wolf Optimizer we get a score of 0.7172586324192401.\n",
            "200\n",
            "Fitting at most 1 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "For Grey Wolf Optimizer we get a score of 0.7269577183886997.\n",
            "250\n",
            "Fitting at most 1 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "For Grey Wolf Optimizer we get a score of 0.7243385725965737.\n",
            "300\n",
            "Fitting at most 1 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "For Grey Wolf Optimizer we get a score of 0.7223028905669284.\n",
            "350\n",
            "Fitting at most 1 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "For Grey Wolf Optimizer we get a score of 0.7185203904364601.\n",
            "400\n",
            "Fitting at most 1 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "For Grey Wolf Optimizer we get a score of 0.7193936114244179.\n",
            "450\n",
            "Fitting at most 1 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "For Grey Wolf Optimizer we get a score of 0.719683368649895.\n",
            "500\n",
            "Fitting at most 1 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "For Grey Wolf Optimizer we get a score of 0.7253086457481664.\n",
            "1\n",
            "50\n",
            "Fitting at most 1 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "For Bat Algorithm we get a score of 0.7008714814102616.\n",
            "100\n",
            "Fitting at most 1 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "For Bat Algorithm we get a score of 0.7239538438319981.\n",
            "150\n",
            "Fitting at most 1 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "For Bat Algorithm we get a score of 0.722107775771256.\n",
            "200\n",
            "Fitting at most 1 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "For Bat Algorithm we get a score of 0.717356142801463.\n",
            "250\n",
            "Fitting at most 1 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "For Bat Algorithm we get a score of 0.7164830628603454.\n",
            "300\n",
            "Fitting at most 1 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "For Bat Algorithm we get a score of 0.7200715295542496.\n",
            "350\n",
            "Fitting at most 1 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "For Bat Algorithm we get a score of 0.7220109236076209.\n",
            "400\n",
            "Fitting at most 1 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "For Bat Algorithm we get a score of 0.7202660801625611.\n",
            "450\n",
            "Fitting at most 1 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "For Bat Algorithm we get a score of 0.7219144005532797.\n",
            "500\n",
            "Fitting at most 1 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "For Bat Algorithm we get a score of 0.7226905342995356.\n",
            "2\n",
            "50\n",
            "Fitting at most 1 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "For Hybrid Bat Algorithm we get a score of 0.7128970409783385.\n",
            "100\n",
            "Fitting at most 1 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "For Hybrid Bat Algorithm we get a score of 0.7212347428457516.\n",
            "150\n",
            "Fitting at most 1 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "For Hybrid Bat Algorithm we get a score of 0.7195875978453684.\n",
            "200\n",
            "Fitting at most 1 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "For Hybrid Bat Algorithm we get a score of 0.717066620654053.\n",
            "250\n",
            "Fitting at most 1 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "For Hybrid Bat Algorithm we get a score of 0.7223015271141394.\n",
            "300\n",
            "Fitting at most 1 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "For Hybrid Bat Algorithm we get a score of 0.7207509521837105.\n",
            "350\n",
            "Fitting at most 1 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "For Hybrid Bat Algorithm we get a score of 0.7193937994868717.\n",
            "400\n",
            "Fitting at most 1 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "For Hybrid Bat Algorithm we get a score of 0.7214288703135423.\n",
            "450\n",
            "Fitting at most 1 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "For Hybrid Bat Algorithm we get a score of 0.7225938231827408.\n",
            "500\n",
            "Fitting at most 1 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "For Hybrid Bat Algorithm we get a score of 0.719102725824219.\n",
            "3\n",
            "50\n",
            "Fitting at most 1 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "For Firefly Algorithm we get a score of 0.7186192642714719.\n",
            "100\n",
            "Fitting at most 1 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "For Firefly Algorithm we get a score of 0.714350293588998.\n",
            "150\n",
            "Fitting at most 1 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "For Firefly Algorithm we get a score of 0.7204603486771923.\n",
            "200\n",
            "Fitting at most 1 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "For Firefly Algorithm we get a score of 0.714447709939994.\n",
            "250\n",
            "Fitting at most 1 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "For Firefly Algorithm we get a score of 0.7220128982633842.\n",
            "300\n",
            "Fitting at most 1 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "For Firefly Algorithm we get a score of 0.721236482423448.\n",
            "350\n",
            "Fitting at most 1 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "For Firefly Algorithm we get a score of 0.7186170545376414.\n",
            "400\n",
            "Fitting at most 1 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "For Firefly Algorithm we get a score of 0.724241203261191.\n",
            "450\n",
            "Fitting at most 1 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "For Firefly Algorithm we get a score of 0.7144468166433391.\n",
            "500\n",
            "Fitting at most 1 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "For Firefly Algorithm we get a score of 0.7233693457260222.\n",
            "4\n",
            "50\n",
            "Fitting at most 1 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "For Hybrid Self Adaptive Bat Algorithm we get a score of 0.7160970646742075.\n",
            "100\n",
            "Fitting at most 1 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "For Hybrid Self Adaptive Bat Algorithm we get a score of 0.7193929061902168.\n",
            "150\n",
            "Fitting at most 1 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "For Hybrid Self Adaptive Bat Algorithm we get a score of 0.7174533240743919.\n",
            "200\n",
            "Fitting at most 1 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "For Hybrid Self Adaptive Bat Algorithm we get a score of 0.7277329588383006.\n",
            "250\n",
            "Fitting at most 1 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "For Hybrid Self Adaptive Bat Algorithm we get a score of 0.7197807850008909.\n",
            "300\n",
            "Fitting at most 1 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "For Hybrid Self Adaptive Bat Algorithm we get a score of 0.7197819603912262.\n",
            "350\n",
            "Fitting at most 1 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "For Hybrid Self Adaptive Bat Algorithm we get a score of 0.7198773550708454.\n",
            "400\n",
            "Fitting at most 1 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "For Hybrid Self Adaptive Bat Algorithm we get a score of 0.7198774491020723.\n",
            "450\n",
            "Fitting at most 1 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "For Hybrid Self Adaptive Bat Algorithm we get a score of 0.7176474045265693.\n",
            "500\n",
            "Fitting at most 1 candidates\n",
            "Optimization finished, 1 candidates were fitted\n",
            "For Hybrid Self Adaptive Bat Algorithm we get a score of 0.7193931882838973.\n"
          ]
        }
      ],
      "source": [
        "MyDict = dict()\n",
        "My_prev_idx = 0\n",
        "for M_type in range(5):\n",
        "    print(M_type)\n",
        "    for Para in range(50,501,50):\n",
        "        print(Para)\n",
        "        start_time = dt.now()\n",
        "        Name, Score = Model_try(DataFile,M_type,Para)\n",
        "        end_time = dt.now()\n",
        "        #total,cpu,monitor,disk,base, time_taken, end_idx\n",
        "        total,cpu,monitor,disk,base, time_taken, My_prev_idx = Get_Time_Energy(start_time,end_time,prev_idx=My_prev_idx)\n",
        "        TempDict = {'Total Energy (J)' : total, 'Total CPU  Energy (J)' : cpu, 'Total Monitor Energy (J)' : monitor, 'Total Disk Energy (J)' : disk, 'Base Energy (J)' : base, 'Time (Sec)' : time_taken, 'Score' : Score*100, 'n_estimators' : Para}\n",
        "        if Name in MyDict:\n",
        "            MyDict[Name].append(TempDict)\n",
        "        else:\n",
        "            MyDict[Name] = [TempDict]\n",
        "        with open('Kaito Kids Dairy.json','w') as kid:\n",
        "            json.dump(MyDict,kid)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json \n",
        "import numpy as np \n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "MyDict = dict()\n",
        "with open('Kaito Kids Dairy.json','r') as kid:\n",
        "    MyDict = json.load(kid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "name_lst = []\n",
        "TE_lst = []\n",
        "CPU_lst = []\n",
        "Mon_lst = []\n",
        "Disk_lst = []\n",
        "B_lst = []\n",
        "Time_lst = []\n",
        "Acc_lst = []\n",
        "n_estimator_lst = []\n",
        "for i in MyDict.keys():\n",
        "    curr_lst = MyDict[i]\n",
        "    for j in range(len(curr_lst)):\n",
        "        inner_dict = curr_lst[j]\n",
        "        \n",
        "        name_lst.append(i)\n",
        "        TE_lst.append(inner_dict['Total Energy (J)'])\n",
        "        CPU_lst.append(inner_dict['Total CPU  Energy (J)'])\n",
        "        Mon_lst.append(inner_dict['Total Monitor Energy (J)'])\n",
        "        Disk_lst.append(inner_dict['Total Disk Energy (J)'])\n",
        "        Time_lst.append(inner_dict['Time (Sec)'])\n",
        "        B_lst.append(inner_dict['Base Energy (J)'])\n",
        "        Acc_lst.append(inner_dict['Score'])\n",
        "        n_estimator_lst.append(inner_dict['n_estimators'])\n",
        "\n",
        "\n",
        "Final_dict = {'Algo Name':name_lst, 'n_estimators' : n_estimator_lst, 'Time Taken' : Time_lst, 'Accuracy' : Acc_lst,'Total Energy (J)':TE_lst,'Base Energy (J)' : B_lst,'CPU Energy (J)' : CPU_lst, 'Monitor Energy (J)' : Mon_lst, 'Disk Energy (J)' : Disk_lst }\n",
        "\n",
        "\n",
        "df = pd.DataFrame(data=Final_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    os.mkdir('Result')\n",
        "except:\n",
        "    pass\n",
        "df.to_csv('Result\\\\Full_Result.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5, 10)"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "algo_gap = len(MyDict.keys())\n",
        "iteration_gap = len(MyDict[list(MyDict.keys())[1]])\n",
        "algo_gap,iteration_gap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    os.mkdir('Result\\\\Algo')\n",
        "except:\n",
        "    pass\n",
        "try:\n",
        "    os.mkdir('Result\\\\Estimator')\n",
        "except:\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(algo_gap):\n",
        "    Name = Final_dict['Algo Name'][i*iteration_gap]\n",
        "    TempDict = {'n_estimators' : n_estimator_lst[i*10:(i+1)*10], 'Time Taken' : Time_lst[i*10:(i+1)*10], 'Accuracy' : Acc_lst[i*10:(i+1)*10],'Total Energy (J)':TE_lst[i*10:(i+1)*10],'Base Energy (J)' : B_lst[i*10:(i+1)*10],'CPU Energy (J)' : CPU_lst[i*10:(i+1)*10], 'Monitor Energy (J)' : Mon_lst[i*10:(i+1)*10], 'Disk Energy (J)' : Disk_lst[i*10:(i+1)*10] }\n",
        "    temp_df = pd.DataFrame(data=TempDict)\n",
        "    temp_df.to_csv(f'Result\\\\Algo\\\\{Name}.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(iteration_gap):\n",
        "    estimators = Final_dict['n_estimators'][i]\n",
        "    name_lst_temp = []\n",
        "    TE_lst_temp = []\n",
        "    CPU_lst_temp = []\n",
        "    Mon_lst_temp = []\n",
        "    Disk_lst_temp = []\n",
        "    B_lst_temp = []\n",
        "    Time_lst_temp = []\n",
        "    Acc_lst_temp = []\n",
        "    for j in range(algo_gap):\n",
        "        name_lst_temp.append(name_lst[(j*iteration_gap)+i])\n",
        "        TE_lst_temp.append(TE_lst[(j*iteration_gap)+i])\n",
        "        CPU_lst_temp.append(CPU_lst[(j*iteration_gap)+i])\n",
        "        Mon_lst_temp.append(Mon_lst[(j*iteration_gap)+i])\n",
        "        Disk_lst_temp.append(Disk_lst[(j*iteration_gap)+i])\n",
        "        B_lst_temp.append(B_lst[(j*iteration_gap)+i])\n",
        "        Time_lst_temp.append(Time_lst[(j*iteration_gap)+i])\n",
        "        Acc_lst_temp.append(Acc_lst[(j*iteration_gap)+i])\n",
        "\n",
        "    TempDict = {'Algo Name' : name_lst_temp, 'Time Taken' : Time_lst_temp, 'Accuracy' : Acc_lst_temp,'Total Energy (J)':TE_lst_temp,'Base Energy (J)' : B_lst_temp,'CPU Energy (J)' : CPU_lst_temp, 'Monitor Energy (J)' : Mon_lst_temp, 'Disk Energy (J)' : Disk_lst_temp }\n",
        "    \n",
        "    temp_df = pd.DataFrame(data=TempDict)\n",
        "    temp_df.to_csv(f'Result\\\\Estimator\\\\{estimators}.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "co2 switch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "name_lst = []\n",
        "TE_lst = []\n",
        "CPU_lst = []\n",
        "Mon_lst = []\n",
        "Disk_lst = []\n",
        "B_lst = []\n",
        "Time_lst = []\n",
        "Acc_lst = []\n",
        "n_estimator_lst = []\n",
        "for i in MyDict.keys():\n",
        "    curr_lst = MyDict[i]\n",
        "    for j in range(len(curr_lst)):\n",
        "        inner_dict = curr_lst[j]\n",
        "        \n",
        "        name_lst.append(i)\n",
        "        TE_lst.append((17.0/72.0)*inner_dict['Total Energy (J)'])\n",
        "        CPU_lst.append((17.0/72.0)*inner_dict['Total CPU  Energy (J)'])\n",
        "        Mon_lst.append((17.0/72.0)*inner_dict['Total Monitor Energy (J)'])\n",
        "        Disk_lst.append((17.0/72.0)*inner_dict['Total Disk Energy (J)'])\n",
        "        Time_lst.append(inner_dict['Time (Sec)'])\n",
        "        B_lst.append((17.0/72.0)*inner_dict['Base Energy (J)'])\n",
        "        Acc_lst.append(inner_dict['Score'])\n",
        "        n_estimator_lst.append(inner_dict['n_estimators'])\n",
        "\n",
        "\n",
        "Final_dict = {'Algo Name':name_lst, 'n_estimators' : n_estimator_lst, 'Time Taken' : Time_lst, 'Accuracy' : Acc_lst,'Total CO2 Emission (mg)':TE_lst,'Base CO2 Emission (mg)' : B_lst,'CPU CO2 Emission (mg)' : CPU_lst, 'Monitor CO2 Emission (mg)' : Mon_lst, 'Disk CO2 Emission (mg)' : Disk_lst }\n",
        "\n",
        "\n",
        "df = pd.DataFrame(data=Final_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    os.mkdir('ResultCO2')\n",
        "except:\n",
        "    pass\n",
        "df.to_csv('ResultCO2\\\\Full_Result.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5, 10)"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "algo_gap = len(MyDict.keys())\n",
        "iteration_gap = len(MyDict[list(MyDict.keys())[1]])\n",
        "algo_gap,iteration_gap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    os.mkdir('ResultCO2\\\\Algo')\n",
        "except:\n",
        "    pass\n",
        "try:\n",
        "    os.mkdir('ResultCO2\\\\Estimator')\n",
        "except:\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(algo_gap):\n",
        "    Name = Final_dict['Algo Name'][i*iteration_gap]\n",
        "    TempDict = {'n_estimators' : n_estimator_lst[i*10:(i+1)*10], 'Time Taken' : Time_lst[i*10:(i+1)*10], 'Accuracy' : Acc_lst[i*10:(i+1)*10],'Total CO2 Emission (mg)':TE_lst[i*10:(i+1)*10],'Base CO2 Emission (mg)' : B_lst[i*10:(i+1)*10],'CPU CO2 Emission (mg)' : CPU_lst[i*10:(i+1)*10], 'Monitor CO2 Emission (mg)' : Mon_lst[i*10:(i+1)*10], 'Disk CO2 Emission (mg)' : Disk_lst[i*10:(i+1)*10] }\n",
        "    temp_df = pd.DataFrame(data=TempDict)\n",
        "    temp_df.to_csv(f'ResultCO2\\\\Algo\\\\{Name}.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(iteration_gap):\n",
        "    estimators = Final_dict['n_estimators'][i]\n",
        "    name_lst_temp = []\n",
        "    TE_lst_temp = []\n",
        "    CPU_lst_temp = []\n",
        "    Mon_lst_temp = []\n",
        "    Disk_lst_temp = []\n",
        "    B_lst_temp = []\n",
        "    Time_lst_temp = []\n",
        "    Acc_lst_temp = []\n",
        "    for j in range(algo_gap):\n",
        "        name_lst_temp.append(name_lst[(j*iteration_gap)+i])\n",
        "        TE_lst_temp.append(TE_lst[(j*iteration_gap)+i])\n",
        "        CPU_lst_temp.append(CPU_lst[(j*iteration_gap)+i])\n",
        "        Mon_lst_temp.append(Mon_lst[(j*iteration_gap)+i])\n",
        "        Disk_lst_temp.append(Disk_lst[(j*iteration_gap)+i])\n",
        "        B_lst_temp.append(B_lst[(j*iteration_gap)+i])\n",
        "        Time_lst_temp.append(Time_lst[(j*iteration_gap)+i])\n",
        "        Acc_lst_temp.append(Acc_lst[(j*iteration_gap)+i])\n",
        "\n",
        "    TempDict = {'Algo Name' : name_lst_temp, 'Time Taken' : Time_lst_temp, 'Accuracy' : Acc_lst_temp,'Total CO2 Emission (mg)':TE_lst_temp,'Base CO2 Emission (mg)' : B_lst_temp,'CPU CO2 Emission (mg)' : CPU_lst_temp, 'Monitor CO2 Emission (mg)' : Mon_lst_temp, 'Disk CO2 Emission (mg)' : Disk_lst_temp }\n",
        "    \n",
        "    temp_df = pd.DataFrame(data=TempDict)\n",
        "    temp_df.to_csv(f'ResultCO2\\\\Estimator\\\\{estimators}.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "'return' outside function (3438313781.py, line 1)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;36m  Input \u001b[1;32mIn [31]\u001b[1;36m\u001b[0m\n\u001b[1;33m    return\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m 'return' outside function\n"
          ]
        }
      ],
      "source": [
        "return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Basement!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Don't go down there!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are only faulty or once used code chunks!!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "score_lst = []\n",
        "estimator_lst = []\n",
        "para_lst = []\n",
        "name_lst = []\n",
        "\n",
        "print(score_lst[-1],estimator_lst[-1],para_lst[-1],name_lst[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-d3IvUgPHrfw",
        "outputId": "99c01bdb-d461-446a-b426-182e7149f33f"
      },
      "outputs": [],
      "source": [
        "score_lst = []\n",
        "estimator_lst = []\n",
        "para_lst = []\n",
        "name_lst = []\n",
        "MyDict = dict()\n",
        "for i in range(5):\n",
        "    ADict, NIA_Name = Model_try(DataFile,i)\n",
        "    MyDict[NIA_Name] = ADict\n",
        "    para_lst.append(ADict['Best Para'])\n",
        "    score_lst.append(ADict['Best Score'])\n",
        "    estimator_lst.append(ADict['Best Estimator'])\n",
        "    name_lst.append(NIA_Name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpAlP65xMi-7"
      },
      "outputs": [],
      "source": [
        "k_lst = list(MyDict.keys())\n",
        "k2_lst = list(MyDict[k_lst[0]].keys())\n",
        "for i in k_lst:\n",
        "  MyDict[i]['Best Estimator'] = None \n",
        "\n",
        "with open(\"NIA_Dicts.json\", \"w\") as outfile:\n",
        "    json.dump(MyDict,outfile) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRdlezKR12gB"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(data= { 'NIA' : name_lst, 'Accuracy' : score_lst, 'Estimator' : estimator_lst, 'Para' : para_lst } )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfYqRPR51TN1"
      },
      "outputs": [],
      "source": [
        "df.to_csv('Final Result.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "DON'T BE UP RUNNING IT PAST 00:00!!\n",
        "YOU SHOULD BE IN BED BY THEN!!!!!!!\n",
        "\n",
        "Here is a bed time story for you \n",
        "\n",
        "\n",
        "She carefully assured her child that there wasn't a monster under the bed, turned off the light and left.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Thanks to her, I now get to enjoy the feast I'd been craving for so long.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 64-bit (microsoft store)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "bcac82fb26a6e7c950421e78519edb87e1a5e005e0aec2fc293e679965bb2493"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
